{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNUbUq6PxP6w1V/KVaxu3Cj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RamyHamrouni/NLP-notebooks/blob/main/Presentation_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BSz5xAFOzN_T"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install langchain_tavily"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1I27L9h7Irc",
        "outputId": "a363cbf5-265e-4471-e907-59c0f46a64b3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_tavily\n",
            "  Downloading langchain_tavily-0.2.12-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.11.14 in /usr/local/lib/python3.12/dist-packages (from langchain_tavily) (3.13.0)\n",
            "Requirement already satisfied: langchain<2.0.0,>=0.3.20 in /usr/local/lib/python3.12/dist-packages (from langchain_tavily) (0.3.27)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=0.3.15 in /usr/local/lib/python3.12/dist-packages (from langchain_tavily) (0.3.79)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.3 in /usr/local/lib/python3.12/dist-packages (from langchain_tavily) (2.32.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain_tavily) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain_tavily) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain_tavily) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain_tavily) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain_tavily) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain_tavily) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain_tavily) (1.22.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain<2.0.0,>=0.3.20->langchain_tavily) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain<2.0.0,>=0.3.20->langchain_tavily) (0.4.35)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain<2.0.0,>=0.3.20->langchain_tavily) (2.11.10)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain<2.0.0,>=0.3.20->langchain_tavily) (2.0.44)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain<2.0.0,>=0.3.20->langchain_tavily) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.15->langchain_tavily) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.15->langchain_tavily) (1.33)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.15->langchain_tavily) (4.15.0)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.15->langchain_tavily) (25.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.3->langchain_tavily) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.3->langchain_tavily) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.3->langchain_tavily) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.3->langchain_tavily) (2025.10.5)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=0.3.15->langchain_tavily) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain<2.0.0,>=0.3.20->langchain_tavily) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain<2.0.0,>=0.3.20->langchain_tavily) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain<2.0.0,>=0.3.20->langchain_tavily) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain<2.0.0,>=0.3.20->langchain_tavily) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.20->langchain_tavily) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.20->langchain_tavily) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.20->langchain_tavily) (0.4.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain<2.0.0,>=0.3.20->langchain_tavily) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain<2.0.0,>=0.3.20->langchain_tavily) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain<2.0.0,>=0.3.20->langchain_tavily) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain<2.0.0,>=0.3.20->langchain_tavily) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain<2.0.0,>=0.3.20->langchain_tavily) (1.3.1)\n",
            "Downloading langchain_tavily-0.2.12-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: langchain_tavily\n",
            "Successfully installed langchain_tavily-0.2.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e96a397"
      },
      "source": [
        "# Task\n",
        "Create a LangGraph orchestrator pattern with dummy tools and run it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab9a5617"
      },
      "source": [
        "## Install langgraph\n",
        "\n",
        "### Subtask:\n",
        "Install the necessary library.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72b47312"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to install the `langgraph` library. This can be done using the `pip install` command in a code cell.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8684381",
        "outputId": "6e4753ab-c6e6-4986-bb2b-ccbc8285675f"
      },
      "source": [
        "%pip install -U langgraph"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langgraph\n",
            "  Downloading langgraph-1.0.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: langchain-core>=0.1 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.3.79)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.1.0 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.1.2-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting langgraph-prebuilt<1.1.0,>=1.0.0 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-1.0.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting langgraph-sdk<0.3.0,>=0.2.2 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.2.9-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langgraph) (2.11.10)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.6.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (0.4.35)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (1.33)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (6.0.3)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (4.15.0)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (25.0)\n",
            "Collecting ormsgpack>=1.10.0 (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph)\n",
            "  Downloading ormsgpack-1.11.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.11.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.4.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core>=0.1->langgraph) (3.0.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (2.32.4)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (0.25.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.3.1)\n",
            "Downloading langgraph-1.0.0-py3-none-any.whl (155 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m155.4/155.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-2.1.2-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-1.0.0-py3-none-any.whl (28 kB)\n",
            "Downloading langgraph_sdk-0.2.9-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.11.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (207 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.6/207.6 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ormsgpack, langgraph-sdk, langgraph-checkpoint, langgraph-prebuilt, langgraph\n",
            "Successfully installed langgraph-1.0.0 langgraph-checkpoint-2.1.2 langgraph-prebuilt-1.0.0 langgraph-sdk-0.2.9 ormsgpack-1.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from openai import OpenAI\n",
        "from typing import List, Dict, Any, Optional, Type, TypeVar\n",
        "from pydantic import BaseModel\n",
        "import json\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "T = TypeVar(\"T\", bound=BaseModel)\n",
        "\n",
        "class LLMClient:\n",
        "    def __init__(self, base_url: str, api_key: str):\n",
        "        self.base_url = base_url\n",
        "        self.api_key = api_key\n",
        "        logger.info(f\"Initialized LLMClient with base_url: {self.base_url}\")\n",
        "\n",
        "    def completion(self, user_input: str, tools: List[Dict[str, Any]] = None) -> Any:\n",
        "        raise NotImplementedError(\"completion must be implemented by subclasses\")\n",
        "\n",
        "\n",
        "class OpenAIClient(LLMClient):\n",
        "    def __init__(self, base_url: str, api_key: str, model: str = \"gpt-4.1-mini\", temperature: float = 0.2):\n",
        "        super().__init__(base_url, api_key)\n",
        "        self.model = model\n",
        "        self.client = OpenAI(api_key=api_key, base_url=base_url)\n",
        "        self.temperature = temperature\n",
        "        logger.info(f\"Initialized OpenAIClient with model: {self.model}, temperature: {self.temperature}\")\n",
        "\n",
        "\n",
        "    def completion(\n",
        "        self,\n",
        "        messages: List[Dict[str, str]],\n",
        "        tools: Optional[List[Dict[str, Any]]] = None,\n",
        "    ) -> Any:\n",
        "        params = {\n",
        "            \"model\": self.model,\n",
        "            \"messages\": messages,\n",
        "            \"temperature\": self.temperature,\n",
        "        }\n",
        "        logger.info(f\"Calling OpenAI completion with params: {params}\")\n",
        "\n",
        "        if tools is not None:\n",
        "            params[\"tools\"] = tools\n",
        "            params[\"tool_choice\"] = \"auto\"\n",
        "            logger.info(f\"Adding tools to completion params: {tools}\")\n",
        "\n",
        "\n",
        "        response = self.client.chat.completions.create(**params)\n",
        "        logger.info(\"Received response from OpenAI completion.\")\n",
        "        return response.choices[0].message\n",
        "\n",
        "    # ðŸ§  Structured output method â€” like LangChainâ€™s with_structured_output\n",
        "    def structured_completion(\n",
        "        self,\n",
        "        messages: List[Dict[str, str]],\n",
        "        schema: Type[T],\n",
        "        tools: Optional[List[Dict[str, Any]]] = None\n",
        "    ) -> T:\n",
        "        \"\"\"\n",
        "        Generate structured output that conforms to a given Pydantic schema.\n",
        "        \"\"\"\n",
        "        logger.info(\"Calling OpenAI structured completion.\")\n",
        "\n",
        "        params = {\n",
        "            \"model\": self.model,\n",
        "            \"messages\": messages,\n",
        "            \"temperature\": self.temperature,\n",
        "        }\n",
        "        if tools is not None:\n",
        "            params[\"tools\"] = tools\n",
        "            params[\"tool_choice\"] = \"auto\"\n",
        "            logger.info(f\"Adding tools to structured completion params: {tools}\")\n",
        "        logger.debug(schema)\n",
        "        #params[\"response_format\"] = schema\n",
        "\n",
        "\n",
        "\n",
        "        logger.info(f\"Structured completion params: {params}\")\n",
        "\n",
        "\n",
        "        response = self.client.chat.completions.create(**params)\n",
        "        logger.info(\"Received response from OpenAI structured completion.\")\n",
        "\n",
        "\n",
        "        content = response.choices[0].message.content\n",
        "        logger.info(f\"Raw structured output: {content}\")\n",
        "        try:\n",
        "            data = json.loads(content)\n",
        "            logger.info(\"Successfully parsed structured output.\")\n",
        "            return schema.model_validate(data)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to parse structured output: {e}\\nRaw output: {content}\")\n",
        "            raise ValueError(f\"Failed to parse structured output: {e}\\nRaw output: {content}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "0MTxz7cM8BZA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47dc51e7-e90d-47c9-d742-d49aed385e4c"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration : 4\n",
            "ChatCompletionMessage(content='', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageFunctionToolCall(id='call_tnZ1XlzQPCQtfSwPbB1wuKHL', function=Function(arguments='{\"query\":\"future of generative AI: trends, challenges, opportunities, ethical concerns, industry impact, technological advancements, regulations, and societal implications\",\"max_results\":6}', name='tavily_search'), type='function', index=0)], reasoning=None)\n",
            "iteration : 3\n",
            "ChatCompletionMessage(content='', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageFunctionToolCall(id='call_dKoraSYGEOjmIcXbAaBQNXUJ', function=Function(arguments='{\"query\":\"future of generative AI: key trends, challenges, opportunities, ethical issues, industry impact, regulations, and technological advancements\",\"max_results\":5}', name='tavily_search'), type='function', index=0)], reasoning=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def strategist_agent(startegist_context_manager,tools,CreativeBrief,tools_executor) -> CreativeBrief:\n",
        "    # Step 1: LLM completes the task, calling web_search internally if needed\n",
        "    llm_client=OpenAIClient(\n",
        "          api_key=\"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\",\n",
        "          base_url=\"https://openrouter.ai/api/v1\",\n",
        "          model=\"gpt-5-mini\",\n",
        "          temperature=0.5\n",
        "    )\n",
        "\n",
        "    response = llm_client.completion(\n",
        "        messages=startegist_context_manager.messages,\n",
        "        tools=tools\n",
        "    )\n",
        "    max_iter = 4\n",
        "\n",
        "    # Step 2: Handle multiple tool calls iteratively\n",
        "    while response.tool_calls and max_iter > 0:\n",
        "        print(\"iteration :\",max_iter)\n",
        "        print(response)\n",
        "        startegist_context_manager.add_message(role=\"assistant\", content=response.tool_calls)\n",
        "        max_iter -= 1\n",
        "        for tool_call in response.tool_calls:\n",
        "            function_name = tool_call.function.name\n",
        "            arguments = json.loads(tool_call.function.arguments)\n",
        "\n",
        "            # Execute the web search\n",
        "            if function_name == \"tavily_search\":\n",
        "                search_results = tools_executor[\"web_search\"](**arguments)\n",
        "\n",
        "                # Aggregate results: attach summaries, titles, URLs\n",
        "                summarized_results = [\n",
        "                    f\"{r['title']}: {r['content']} ({r['url']})\"\n",
        "                    for r in search_results['results']\n",
        "                ]\n",
        "                startegist_context_manager.add_message(\n",
        "                    role=\"assistant\",\n",
        "                    content=f\"Tool called: {json.dumps(summarized_results)}\"\n",
        "                )\n",
        "\n",
        "        # Step 3: Send updated messages back to LLM\n",
        "        response = llm_client.completion(\n",
        "            messages=startegist_context_manager.messages+[{\"role\": \"system\", \"content\": \"Check if all aspects of the topic are covered. If not, perform additional searches and provide new sources. If everything is already covered, confirm coverage.\"}],\n",
        "            tools=tools\n",
        "        )\n",
        "        print(response)\n",
        "\n",
        "\n",
        "    # Step 4: Once all tool calls are resolved, generate structured CreativeBrief\n",
        "    final_brief = llm_client.structured_completion(\n",
        "        messages=startegist_context_manager.messages,\n",
        "        schema=CreativeBrief\n",
        "    )\n",
        "    startegist_context_manager.add_message(role=\"assistant\", content=final_brief)\n",
        "\n",
        "    return final_brief\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-xusS2L2mSKu"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ContextManager:\n",
        "    def __init__(self,messages:List):\n",
        "        self.messages = messages\n",
        "\n",
        "    def add_message(self, role: str, content: str):\n",
        "        self.messages.append({\"role\": role, \"content\": content})"
      ],
      "metadata": {
        "id": "J1_21Nlt997X"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_expanded_context(\n",
        "        base_prompt: str,\n",
        "        role: Optional[str] = None,\n",
        "        examples: Optional[List[str]] = None,\n",
        "        constraints: Optional[List[str]] = None,\n",
        "        audience: Optional[str] = None,\n",
        "        tone: Optional[str] = None,\n",
        "        output_format: Optional[str] = None\n",
        "    ) -> str:\n",
        "          \"\"\"\n",
        "          Create an expanded context from a base prompt with optional components.\n",
        "\n",
        "          Args:\n",
        "              base_prompt: The core instruction or question\n",
        "              role: Who the model should act as\n",
        "              examples: List of example outputs to guide the model\n",
        "              constraints: List of requirements or boundaries\n",
        "              audience: Who the output is intended for\n",
        "              tone: Desired tone of the response\n",
        "              output_format: Specific format requirements\n",
        "\n",
        "          Returns:\n",
        "              Expanded context as a string\n",
        "          \"\"\"\n",
        "          context_parts = []\n",
        "\n",
        "          # Add role if provided\n",
        "          if role:\n",
        "              context_parts.append(f\"You are {role}.\")\n",
        "\n",
        "          # Add base prompt\n",
        "          context_parts.append(base_prompt)\n",
        "\n",
        "          # Add audience if provided\n",
        "          if audience:\n",
        "              context_parts.append(f\"Your response should be suitable for {audience}.\")\n",
        "\n",
        "          # Add tone if provided\n",
        "          if tone:\n",
        "              context_parts.append(f\"Use a {tone} tone in your response.\")\n",
        "\n",
        "          # Add output format if provided\n",
        "          if output_format:\n",
        "              context_parts.append(f\"Format your response as {output_format}.\")\n",
        "\n",
        "          # Add constraints if provided\n",
        "          if constraints and len(constraints) > 0:\n",
        "              context_parts.append(\"Requirements:\")\n",
        "              for constraint in constraints:\n",
        "                  context_parts.append(f\"- {constraint}\")\n",
        "\n",
        "          # Add examples if provided\n",
        "          if examples and len(examples) > 0:\n",
        "              context_parts.append(\"Examples:\")\n",
        "              for i, example in enumerate(examples, 1):\n",
        "                  context_parts.append(f\"Example {i}:\\n{example}\")\n",
        "\n",
        "          # Join all parts with appropriate spacing\n",
        "          expanded_context = \"\\n\\n\".join(context_parts)\n",
        "\n",
        "          return expanded_context"
      ],
      "metadata": {
        "id": "C9RgmE0xY5yS"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6597fa20"
      },
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Get Tavily API key\n",
        "os.environ[\"TAVILY_API_KEY\"] = userdata.get(\"TAVILY_API_KEY\")\n",
        "\n",
        "# Get OpenAI API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tavily_search(query: str, max_results: int = 3):\n",
        "    search = TavilySearch(max_results=max_results)\n",
        "    results = search.invoke(query)\n",
        "    return results\n",
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"tavily_search\",\n",
        "            \"description\": \"Searches the web using Tavily and returns top results for a query.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"query\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The query to search for on the web.\"\n",
        "                    },\n",
        "                    \"max_results\": {\n",
        "                        \"type\": \"integer\",\n",
        "                        \"description\": \"Maximum number of search results to return.\",\n",
        "                        \"default\": 3\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"query\"]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "Qh6o_b6GANFO"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "class CreativeBrief(BaseModel):\n",
        "      target_audience: str | list = Field(description=\"The target audience for the brief\")\n",
        "      primary_goal: str = Field(description=\"The primary goal of the brief\")\n",
        "      professional_tone: str = Field(description=\"The professional tone of the brief\")\n",
        "      key_takeaway: str = Field(description=\"The key takeaway from the brief\")\n",
        "      research_context: dict = Field(description=\"The research context for the brief\")\n"
      ],
      "metadata": {
        "id": "WjRnnUzicSRw"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "STRATEGIST_SYSTEM_PROMPT = create_expanded_context(\n",
        "    base_prompt=\"\"\"\n",
        "    Your job is to take a user's topic and create a 'Creative Brief' for the presentation team.\n",
        "\n",
        "    You must define:\n",
        "      - target_audience\n",
        "      - primary_goal (e.g., 'to inform', 'to persuade', 'to get funding')\n",
        "      - professional_tone (e.g., 'casual', 'formal', 'academic')\n",
        "      - key_takeaway\n",
        "      - research context (deep research gathered from multiple web searches to cover all aspects)\n",
        "    \"\"\",\n",
        "    role=\"You are an expert communications strategist and audience analyst.\",\n",
        "    output_format=\"JSON\",\n",
        "    constraints=\"If the user's input lacks important details, perform the necessary web searches to gather basic context before generating the brief.\"\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "strategist_messages = [\n",
        "    {\"role\": \"system\", \"content\": STRATEGIST_SYSTEM_PROMPT},\n",
        "    {\"role\": \"user\", \"content\": \"Create a presentation about the futur of generative ai, all aspects\"}\n",
        "]\n",
        "tools_executor={\n",
        "    \"web_search\": tavily_search\n",
        "}\n",
        "startegist_context_manager = ContextManager(strategist_messages)\n",
        "strategist_agent=strategist_agent(startegist_context_manager,tools,CreativeBrief=CreativeBrief,tools_executor=tools_executor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIgm-MVSmVm3",
        "outputId": "df4e7ffd-81f3-4ef4-f288-60fe13f8e2f1"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration : 4\n",
            "ChatCompletionMessage(content='', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageFunctionToolCall(id='call_rgiIYSfHo4IWOQqMLtzuK1Is', function=Function(arguments='{\"query\":\"future of generative AI trends 2025 2030 research reports articles\",\"max_results\":5}', name='tavily_search'), type='function', index=0), ChatCompletionMessageFunctionToolCall(id='call_pSxjUej0uQ5yS4SjlzTEMrgE', function=Function(arguments='{\"query\":\"applications of generative AI across industries examples healthcare finance art education manufacturing 2024 2025\",\"max_results\":5}', name='tavily_search'), type='function', index=1), ChatCompletionMessageFunctionToolCall(id='call_8QIWY4EQZHzM6UgMkqeHgrCk', function=Function(arguments='{\"query\":\"risks ethics regulation generative AI deepfakes bias copyright 2024 2025 policy\",\"max_results\":5}', name='tavily_search'), type='function', index=2), ChatCompletionMessageFunctionToolCall(id='call_KUCUylKIT1znGzW2Zf7G0O5M', function=Function(arguments='{\"query\":\"technical advances generative models diffusion models transformers multimodal foundation models 2024 2025 overview\",\"max_results\":5}', name='tavily_search'), type='function', index=3)], reasoning=\"**Preparing creative brief**\\n\\nI need to create a JSON for the creative brief, which should include elements like target audience, primary goal, professional tone, key takeaway, and a detailed research context gathered from multiple web searches. The research context is crucial and must summarize the search results with proper citations. I should prepare queries now and plan to call the function to execute a parallel search with up to five results each. Let's proceed with that!\", reasoning_details=[{'type': 'reasoning.summary', 'summary': \"**Preparing creative brief**\\n\\nI need to create a JSON for the creative brief, which should include elements like target audience, primary goal, professional tone, key takeaway, and a detailed research context gathered from multiple web searches. The research context is crucial and must summarize the search results with proper citations. I should prepare queries now and plan to call the function to execute a parallel search with up to five results each. Let's proceed with that!\", 'format': 'openai-responses-v1', 'index': 0}, {'type': 'reasoning.encrypted', 'data': 'gAAAAABo9rIW2llvM171-Hveo4bDXv6TE6qPS4hoUyqfUPpEOGoR468hr3J0RCaJhJQr6fN2SXqpkYY9oDvRrzonFNCZpNOhzgcwZl4gietQ9A7y_mQ0ZJPXS8F-oVMXINCT8_bckTaRjX-WqFW_y9nTssHFVZUsxljdmu33tD2CfVji8lF51nlmxqZv2kpCv88wBoyZzF2zmiTotNhEQ7vOT--25qUFZ7dSOviYBnDmCbfrwFEUr16jOTOFrxECLEIJq0ws1hhhD06tn2TRoEcCyQxj6OFuYl_5RWbp2yQT8r7ftBOLKh2F3dVQ9m6TgNMsqy7HFBJxwDO69iOYFil7OVar0wgwVIoZbE4hbeWSuXZ-9u5C-goOyl9tiTrg2wTzuPCs2kU_3c333_XlsugcKYtjMeQ0-zbGJry_adEB5cT0szGxBIh05mqqe8uUAUXWjhfDsNLzExDOSzC2Ia5LhXTBJkIeAk6Aqsw5cCYjgKlDBitnrG8dh08WZyt_jH0YyBm0soK4Qzllgcc78rA7pgQMF3dXlmLAtu6dKV0yYE-ZEoBOjLaezE_gvAclxfUaaZ7EKefw4Qpto7bZe4t25nvhotxEdGPXLgjfnkGSaPmQ74chb0UbRVEW7mLRDFWmib8WAl6B969LAJ81dADErm5Q4HVglZioT6gZPNlYVLglafbPaD96VA5hAx-ivAzlu31kPmFS5c2IS44zo4oSWERK7d7ONT_FTVeExb0fVzV2bk2er7VHIvMHq_JWEt2Gbu-EDb-Bcz-SKJqq_yZkUnizrW6Y-yLDkNPjsCGRmuFjg5tDvKNDHysjIWQj9qAjbkdtYCoBMlcck4e4Eh6t1dtG4dFPedoqW8RCIzipjp6f0zKtTfVJypfjicOkDBBbDDmDwEo-995F252E7QFh8jp4OOc7sxk0lAjGXdFwXEn8Q67frcGdBoFnKraizdL2MsTGscZNtXQ6WISyDMfpBHD6-FKlB-plRMVGShV2UxrqQp0e4573X886WmoxLWoGSCBbZvPJ7pNB2NwHLz70wW0YQTcCpl3Gi_dCdtAuKVQ_EOei1h1M0arT0YXkO0yO5_1JZdavHtUN9iU9edEVJCMEymKv02rTIGuyKXFHUlEH4CtMsLLLgu0w_lj18ppfNaj6zyyGCyKdTBOv6H_4syri6OCpGStwSVJv_sW6EixmM-MEUIvqFrGZ4tqwaXFPeV5jtSzrUHu7TwfPgeY_ViHx_OJR2QCVk77UjsilrAD3lEEmKvtdGeAXAOuDesIVR--SNT2jAjALMjupLUtX1Sb1ZFY8rv9YydT6ujxjO6ut2RyP7KtB2x_PvbuO_AuBgEtIggbDEaPsxxMg1ulENTNd8wIvUjPkcpRkOYQEeolTaQX40zKua7Yyr_BZ7cGyxE8RlhwGEFX6z995g5hbp_23c6q1Ysi6OXc0Xm9Q_4RAtRWhLf4TN4RCSmFQOiFnu1JX6tBXGJR3I1x9byx_tRvoSkH02s4hI0tsYEVnjOFPBc1ttBkHdXjL1lRkH47byCVO5_65Vk_9adku1mAbeCBEeqop37m0445pVUNVM7X9NJ-vOiq-yACx31pL8ZPl_ZO3SKk93NDrNVDRllxTDjfqATvCA_3pE124du7jckunDNq8FQf2jFh-m878KSKcJVwaZl23iod0siF-xrpgdUIX_zuIovvJHvt1W1iCU4Ku1OpbqJQTZWKydioZZwZlLHBCLpsm0vM24VHl8ZhrIz-4LkOI_7bcq4REDVmzXEx8rYkimP9CQOeVEGCuWAkcycVAh_RqvO1l7A5ha_RpmZHxyrpTiQQFFGrvWcLBuyIfFrJP1PBEp1QmdWFqcOzNCiaPChzNC9yquByBtek16JFBfem1PDC7SFM4vVE4thsbgh4BxOERr2hJHdOPxLnI8vjZad3o1YfZ555F_wcd27tJq2QOd0hq--4vffY455hB45t4K0IExnoenJdSRSSiL1nYnGkH_GSkaUK3PKV2aPC4Yw-IHIDhpHHZ5ANQeawL0m_M5arKLjaw0Yk7VylDmlPuVjAqTQUUg5T8Q0qXRrYrvQ4fM4ibRte5b8SRosEnAcY-4v38aLXjoZgbdEhP_joxre5KxTfKtzATQFSnHNCAgMZhVCCCfQ5-I1uey6zCONMaeACv-SZ0Pet-GZRVl-_BCZp7UCW6PsvC6B-om8YH2FDN7rZuE7akiWYCwX4HRwzIDeQ1W6qletIDkPpNnlQQEAqYmEwhe4BQveSnXFUozZPGLV_2jZGGr6RWo6oI_erf9jutPPEkQTbT3Fki0XEpSQGUZwuoScYx3HGfZnSNKloPjaiZtDcQg1BSofz7jYET4xthUSreLr6VjvlgRvVwlro8KXBTkw56-hBT8ut79DNUl_-zMkBGIVTuJkY7c-MjA9jyiBVtq384ZFKHizGl2v6WWoFkFDPoxb5GzJ3Qza1FvuW0lH_OO2ZRngmK9JaXMX70YAy8zq9QQ8qzy50KiXpTN09gcToyTRa09z7yo-ncT2V10pb2vGYgJ8uW3m-nehI9JYqj2ou9N7q2QNyVnHALaq91iZ7hdUufDlJOmEMwYjTBUW-w1DFHOMZVtAX1I19tz4lNDzRreIMxaqCFIdbkFw1C-KZsp-dNz4Ktsbt87JMTVFl07g_y0VIN_gFlLPDet2pFrftmvU2KUBvtk9vxB93HvH8W4rn1iTE97nYh1dmFJe4hNcwqx6A9h4XMw2_lp59dwjy2l3FGWMzYEyYUVx5Q1dyg_cz1jeYZzx0QTZHuqpgX6bTUIv_v7N7-zn_3Wq9KEVYk7b8mxz4kFLKAELWwOEkJqYrVVAfeR9MrZg==', 'id': 'rs_02dd6f0828ef048d0168f6b20e3cc48195844b784bae2ae222', 'format': 'openai-responses-v1', 'index': 0}])\n",
            "ChatCompletionMessage(content='', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageFunctionToolCall(id='call_02Zrdi7hEbRdUGhAJ0pOWTif', function=Function(arguments='{\"query\":\"generative AI future trends 2025 2030 report\",\"max_results\":3}', name='tavily_search'), type='function', index=0), ChatCompletionMessageFunctionToolCall(id='call_AyMAxuYgKEI7QqueWh3RNQOY', function=Function(arguments='{\"query\":\"generative AI impact on jobs economy productivity 2024 2025 study PwC Wharton Acemoglu\",\"max_results\":3}', name='tavily_search'), type='function', index=1), ChatCompletionMessageFunctionToolCall(id='call_jOw0syOG3Mvq4HMAIt3Zy3G1', function=Function(arguments='{\"query\":\"ethical challenges generative AI deepfakes regulation 2024 2025 legislation overview\",\"max_results\":3}', name='tavily_search'), type='function', index=2), ChatCompletionMessageFunctionToolCall(id='call_WsZpIpwRbNgEC3A4ZeuR0NOE', function=Function(arguments='{\"query\":\"technical architectures diffusion models transformers multimodal 2024 2025 research overview\",\"max_results\":3}', name='tavily_search'), type='function', index=3), ChatCompletionMessageFunctionToolCall(id='call_y8qvqEltNDYDdVsQPf3Ynpe1', function=Function(arguments='{\"query\":\"agentic AI autonomous AI agents trends 2024 2025 companies\",\"max_results\":3}', name='tavily_search'), type='function', index=4), ChatCompletionMessageFunctionToolCall(id='call_kvO6nH43lF8M3oclqyBZrN62', function=Function(arguments='{\"query\":\"generative AI applications across industries healthcare finance education entertainment 2025 examples\",\"max_results\":3}', name='tavily_search'), type='function', index=5), ChatCompletionMessageFunctionToolCall(id='call_7lyXmlDx62pv0uxHO7udCTXu', function=Function(arguments='{\"query\":\"AI compute energy consumption environmental impact large models 2024 2025\",\"max_results\":3}', name='tavily_search'), type='function', index=6), ChatCompletionMessageFunctionToolCall(id='call_S8D3FNy2Iy2v4k1E6qSaZqPK', function=Function(arguments='{\"query\":\"open source vs proprietary generative AI landscape 2024 2025 Hugging Face Mistral LLaMA OpenAI Anthropic\",\"max_results\":3}', name='tavily_search'), type='function', index=7)], reasoning='**Searching for generative AI insights**\\n\\nI think we should run multiple search queries to gather comprehensive research context on generative AI. This includes examining technical foundations, industry applications, economic impacts, ethics, safety, and future trends.\\n\\nIâ€™m considering queries like \"generative AI future trends 2025 2030\" and \"impact on jobs and economy.\" I\\'ll also look into ethical challenges, technical architectures, and energy consumption related to AI models. Overall, Iâ€™m excited to collect all this information through parallel searches!', reasoning_details=[{'type': 'reasoning.summary', 'summary': '**Searching for generative AI insights**\\n\\nI think we should run multiple search queries to gather comprehensive research context on generative AI. This includes examining technical foundations, industry applications, economic impacts, ethics, safety, and future trends.\\n\\nIâ€™m considering queries like \"generative AI future trends 2025 2030\" and \"impact on jobs and economy.\" I\\'ll also look into ethical challenges, technical architectures, and energy consumption related to AI models. Overall, Iâ€™m excited to collect all this information through parallel searches!', 'format': 'openai-responses-v1', 'index': 0}, {'type': 'reasoning.encrypted', 'data': 'gAAAAABo9rIs3ocA2-nINBNQHLaa_5o4QSsKCF8wqrJLv5YiQtsTfWj7gkhCs58IJH1dc5i_DHFDIwojyqeLJpQ5JkZrXOZTdJljN6jprCohJ0sQN2u_OfbyV_FJVWXPLiuFPNIKbiEiwQcfhvBd8AdNSZXiT3-P07MZIuyPyCGMJdBiTH_dWq58-plUnO6c8XuzCmB3bHcGreVeqqkPWjV-RQ4XeH0l81BEWXYkk_ZpT4UQFMhCNkJ5un2qSF4P9DChl_VUJlw0nqlT6g7YjITOAFbn57rpgJassNaWaEwffFRtezHWyFPqienZjtrM9xcMMxAeCcV7SzzfNFdXmQilxV1hCrgMRu3aJ6fiQFUz_sHrALgHLxam2av-7lO_mnETtbtnO7sw-cKkiD3oc-ytvqnLk7baAIhXuMwpPt3x7V4tOrwrDbzk_e51ZiUxLVWKuwPslsbDamjjRVgYiEz9ey6Mqf3GcV_H0_7BFvGpWg9QwRV_1bIL2xL-JWBiwcImFRoWeVHfmVdNyGZ5l-6JnwtwHb-cIdL5dQY8QAZVKGH5dTrDWeDYq2JBKvWk1ON9b4qGGqUZF7r3-xplNivhZCopv1Q1qC2x6ram5nkOaUbaShZpbZ3WD_Mgu0Oz87ZQ5ZiiTAqMj4AuSoTGK8-Z-rqV10X-767O6rEvYNuDig5XGy40ob4N7zvXffu4rr7aCZdKl9FcMDBqBMUwojar0UakIypaCY5W-rbQxvivuSytEsY0yqWOnRgjzoG_iT_qDiqB85uxEq1JTuDQbGYqT-KfsLbGjbmFjwGQLroU2UxZU66KNM-y5KR6wKXYeXw1upveNSR4SuDVpeN2ggbgz9up4IjVup7Oz_5V3I1p6TwrJ8Z1-ST4crillU0-QPYWuVMCJ8Am-vSlz3bm3SOzRajUfVpyDtzH1zzpHZ2Ctw4q8p0eqey4KqOr0-2W_MuByoHqEFlwasgKmFtVe-De5l1TA-K_F3emN8ti2eEcDZWefGVDoFabbYfaRojMUm_tQAFO56wscfdPp8e_rD_oAObNHZ4KKiDDFqnv5GbEi2qnJr1dqu1SRLzAvHqRqIecIi-vjKyyAVXye_uQKRzMPsl5oNy_ERXI21KtwPnPNJ_gQeZIh5V_rzLPNFJIBUfXnImjJptzTvoJEWYfzYY-1GA2PmpG_45uCL82iqEU7WRmZ9q7-iolY5zrtDdBC15MGkMsMRL9tXzGZkC_RBbaCkWpGcJNcALNhBJXRpAo2-yzizLGd-9yadqHJGr39fK0YMNtTGaOFOgP1FR0X6QzzmkUe00b3L9LZTKP9Bx5k5o9aB8e9Yt5P4vR6OBZC2HydtE2PX_J1CU0BhhibgN1633SOwrM_5O7JnQFS0HyrhsVRbERAKlcmF7Trm4B3KMw4H-Fuwy9FpEuYo7_rGybNIHLK_UNlbg0QDx7eD9Ly3YT9D8F25Z_DdhAqJaPyaLF779UAb8ArIyfDNK_Pda4qc7GYsADPg69yvfFOxPpro8PhVYrCwjAucNFp--_NszKvA6EcHDYYzIbGpr1UXJFlJ0Yl4RYqHQEy8ulaV9zKSko_RHTGIC_iC-xhY3IiDr766eb2bhkgPJaADnq0rc-bvH8oH7nr09DLxCJH9LqAHjohADHTLD1pbBlYLtZRiQMmkD7ga9y1eibBHHQO2UdduXeiVkZsiuGd6WVGBoQUVDXRc4uH2aHt90LGbBLBiqksFXIFBpiD9puqE1cqvafc1AlH76_NHr2kq0NoRuU-6D1n3UThtpEELDgLSBD_E63XvyJGHWBC2vpA3xo3p7_cf6q1-HEtGle3I7PBBk5aiMZEGa0Zas1tBINjBijxQi9Lap5DrY2aH-gGD6WaWZKvUlK2Ha7fqZYLQ0cf-IPtf7HcfNLF7gnjaBRtnkwmazes5HSJrPhkEf17URf0wZzjWksKmotTT6w9X1zNWX7V_FCH3fmLdMkNx3HLCHaUZODD-zXZ6OZJRegUIcwBYzswwnjrbn4ZO8TAGLNSFb-MrDqAQeM7G38kgnSwlam6oJs_Pko_5nnKKIzFjrxjJcJURZFRh1I-0JytTki8Ca-zhYJ5JoZmRuuYoZApzy083-wqAbO_gQjNrKLVV6Cjlmsn8CxDrlHewfYAwit3T-t3wAtiuB-PWQnJuhf9UIiqjTYjetaN42yGFsdVphI5_XusLKyFbNW_t5lu6mM-bPw5c2mtYOGUFmDmnTJveBLRr6Gg5CmdgqFYtEWlCHxXzBWcLPqMmiGctUP2Ive0tb91bBKWzPIcu-SMOdz0tAqmm17ZB65DOE8C0J9IqJ0JvKJkODExLNjBHz8ggnXHzbDrhBGRtB9JNcjWdM9X219S_eGZXUNZ9x4bgHxwAWhYyy86DpWorSX7nER0-MC3eG285VHuPxZnGvlvs-4izBLuTVILSHA0pXH_WXxt3hsg1CqgPfEeYVGAHya1JJQ2LsTMVOYPyjB38VbcpLCWC-NmyLMCl9hWUiB5SqozFGcEpcFKtUMFLQO0ZhrgToL3v9xC9oGc40_4UFcduNWtjDOjM1zvtVaRrQpwCJBQo5A3HQXylAJx_PusORBKSHRehhvSCkkrZX5DE2lT26A2vrtHG-nbrFmSkshvFaI-B8SwiKiEasENjTvVBLcba70gu2jrZ86q8-pCu4drU9iJUqcKl3LKAPj08O8_7zIwmzcQmfkNecG3mKxicdj36eD9-TAP2atTCMGL7nfOQYmPJQeyz_rY7p7ITx2y_GgY8kM7kyY1wmHtYmWB8PODYRsVVlY4PvjlymlEJDQzKEqAWVA3XscPvC_oFIeVjwZlmX3kQGN0fHahLsMwRUU8wFAQdzWsLnIRJmvfRQvNTrEra_ZpvBi3Y-a7wWFBI9aNrn8uwvIZ2R8W9mrTFTFYdAiLDr5t0KmpDKF6Ye7KtXG9PcwKsjkQRLe9aOxAML6Mcop3ev_sysEBsxfEVURp281io3LZXzgp-Hn97XEfANxHuUQDDkhe1EH8lJlXLqz7lF8N4501v_gTtNg4_5eIH7cyR5edd98q87D6_R5NIA3JqSto632F16STBnidHSuBI6o9ir5nZdgQZCxY6dN1tuQmwdE6Y-L-HWM6OsBSoHqe_rMS_x_KMth5oj810_Lph1hYYbBW42-QRwT6k6Ci087SAkHHteIQR2wIbjwZg3rDBmb9VXI5nfLHA1JV9ewCZJ9oy9yyT-ZO3VN-f1LHOlGa1x-xXF1L52pOKh9SHq1TJlYQginsDuLwLDTSzqN4r7rtn2P10W61MgJcFZVhGKcZrDisE_LX6xklWSkSeYyNGXiRpZIkhw-Sb203oEYJj6PaQmSLAr-kHCNrT30QnRqP_XNR_k8rKMrCwlYx4K5ANXDuN3zvsRzth3run79AB93IitfvClrd_W-48qDLsI0sdHNB1n4lrKsVHZQ3ZlHKnLHWaaWKt64RVThXxUjvJTrQa3z-NDaGs-l9nESUCjqUUPAiW0s9uBAfplJs6CCc_KW8WSX67bF-OOqokZf82X9UFNcmtfjTMDINVmMJBXMKh3m2DEALC7PQcfhK8Tgp2hvsXlTaKxrpiP-Vvc0COlHPxaZcf18PGYL1UUiqOGNk8sQXERjSbNpDkbZMICg2HKMo7JvH-dV4eg3mJUYHVyOL_lLk4HOnq88GKNE6vrE7XjFn91VPpMU21wyniqiGTQMtmaZUA3oOSZS0-g3mVCqpkyc9jQUSbtmXKYssd5tAXcI65L_izeuyj1lm2bHC6pbUHiWtew1_76RNqirRo2V5uwRPiEQyg8DafBGyM6EUndYe8yRtjU-6MknXMPTWScWZthnnhjq0Cg7EV5yXRZTl11onpqkKb8VK1rtnt32FABowRw976ernkh4y-PSjMiUXeFQXIjfzymW0M_SUuq44ZKHxDdwXS2xjev1PQX1P2QlwGQNPwv5PiM=', 'id': 'rs_68f6b220dd3481909bd1d7423ee7e3f6', 'format': 'openai-responses-v1', 'index': 0}])\n",
            "iteration : 3\n",
            "ChatCompletionMessage(content='', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageFunctionToolCall(id='call_02Zrdi7hEbRdUGhAJ0pOWTif', function=Function(arguments='{\"query\":\"generative AI future trends 2025 2030 report\",\"max_results\":3}', name='tavily_search'), type='function', index=0), ChatCompletionMessageFunctionToolCall(id='call_AyMAxuYgKEI7QqueWh3RNQOY', function=Function(arguments='{\"query\":\"generative AI impact on jobs economy productivity 2024 2025 study PwC Wharton Acemoglu\",\"max_results\":3}', name='tavily_search'), type='function', index=1), ChatCompletionMessageFunctionToolCall(id='call_jOw0syOG3Mvq4HMAIt3Zy3G1', function=Function(arguments='{\"query\":\"ethical challenges generative AI deepfakes regulation 2024 2025 legislation overview\",\"max_results\":3}', name='tavily_search'), type='function', index=2), ChatCompletionMessageFunctionToolCall(id='call_WsZpIpwRbNgEC3A4ZeuR0NOE', function=Function(arguments='{\"query\":\"technical architectures diffusion models transformers multimodal 2024 2025 research overview\",\"max_results\":3}', name='tavily_search'), type='function', index=3), ChatCompletionMessageFunctionToolCall(id='call_y8qvqEltNDYDdVsQPf3Ynpe1', function=Function(arguments='{\"query\":\"agentic AI autonomous AI agents trends 2024 2025 companies\",\"max_results\":3}', name='tavily_search'), type='function', index=4), ChatCompletionMessageFunctionToolCall(id='call_kvO6nH43lF8M3oclqyBZrN62', function=Function(arguments='{\"query\":\"generative AI applications across industries healthcare finance education entertainment 2025 examples\",\"max_results\":3}', name='tavily_search'), type='function', index=5), ChatCompletionMessageFunctionToolCall(id='call_7lyXmlDx62pv0uxHO7udCTXu', function=Function(arguments='{\"query\":\"AI compute energy consumption environmental impact large models 2024 2025\",\"max_results\":3}', name='tavily_search'), type='function', index=6), ChatCompletionMessageFunctionToolCall(id='call_S8D3FNy2Iy2v4k1E6qSaZqPK', function=Function(arguments='{\"query\":\"open source vs proprietary generative AI landscape 2024 2025 Hugging Face Mistral LLaMA OpenAI Anthropic\",\"max_results\":3}', name='tavily_search'), type='function', index=7)], reasoning='**Searching for generative AI insights**\\n\\nI think we should run multiple search queries to gather comprehensive research context on generative AI. This includes examining technical foundations, industry applications, economic impacts, ethics, safety, and future trends.\\n\\nIâ€™m considering queries like \"generative AI future trends 2025 2030\" and \"impact on jobs and economy.\" I\\'ll also look into ethical challenges, technical architectures, and energy consumption related to AI models. Overall, Iâ€™m excited to collect all this information through parallel searches!', reasoning_details=[{'type': 'reasoning.summary', 'summary': '**Searching for generative AI insights**\\n\\nI think we should run multiple search queries to gather comprehensive research context on generative AI. This includes examining technical foundations, industry applications, economic impacts, ethics, safety, and future trends.\\n\\nIâ€™m considering queries like \"generative AI future trends 2025 2030\" and \"impact on jobs and economy.\" I\\'ll also look into ethical challenges, technical architectures, and energy consumption related to AI models. Overall, Iâ€™m excited to collect all this information through parallel searches!', 'format': 'openai-responses-v1', 'index': 0}, {'type': 'reasoning.encrypted', 'data': 'gAAAAABo9rIs3ocA2-nINBNQHLaa_5o4QSsKCF8wqrJLv5YiQtsTfWj7gkhCs58IJH1dc5i_DHFDIwojyqeLJpQ5JkZrXOZTdJljN6jprCohJ0sQN2u_OfbyV_FJVWXPLiuFPNIKbiEiwQcfhvBd8AdNSZXiT3-P07MZIuyPyCGMJdBiTH_dWq58-plUnO6c8XuzCmB3bHcGreVeqqkPWjV-RQ4XeH0l81BEWXYkk_ZpT4UQFMhCNkJ5un2qSF4P9DChl_VUJlw0nqlT6g7YjITOAFbn57rpgJassNaWaEwffFRtezHWyFPqienZjtrM9xcMMxAeCcV7SzzfNFdXmQilxV1hCrgMRu3aJ6fiQFUz_sHrALgHLxam2av-7lO_mnETtbtnO7sw-cKkiD3oc-ytvqnLk7baAIhXuMwpPt3x7V4tOrwrDbzk_e51ZiUxLVWKuwPslsbDamjjRVgYiEz9ey6Mqf3GcV_H0_7BFvGpWg9QwRV_1bIL2xL-JWBiwcImFRoWeVHfmVdNyGZ5l-6JnwtwHb-cIdL5dQY8QAZVKGH5dTrDWeDYq2JBKvWk1ON9b4qGGqUZF7r3-xplNivhZCopv1Q1qC2x6ram5nkOaUbaShZpbZ3WD_Mgu0Oz87ZQ5ZiiTAqMj4AuSoTGK8-Z-rqV10X-767O6rEvYNuDig5XGy40ob4N7zvXffu4rr7aCZdKl9FcMDBqBMUwojar0UakIypaCY5W-rbQxvivuSytEsY0yqWOnRgjzoG_iT_qDiqB85uxEq1JTuDQbGYqT-KfsLbGjbmFjwGQLroU2UxZU66KNM-y5KR6wKXYeXw1upveNSR4SuDVpeN2ggbgz9up4IjVup7Oz_5V3I1p6TwrJ8Z1-ST4crillU0-QPYWuVMCJ8Am-vSlz3bm3SOzRajUfVpyDtzH1zzpHZ2Ctw4q8p0eqey4KqOr0-2W_MuByoHqEFlwasgKmFtVe-De5l1TA-K_F3emN8ti2eEcDZWefGVDoFabbYfaRojMUm_tQAFO56wscfdPp8e_rD_oAObNHZ4KKiDDFqnv5GbEi2qnJr1dqu1SRLzAvHqRqIecIi-vjKyyAVXye_uQKRzMPsl5oNy_ERXI21KtwPnPNJ_gQeZIh5V_rzLPNFJIBUfXnImjJptzTvoJEWYfzYY-1GA2PmpG_45uCL82iqEU7WRmZ9q7-iolY5zrtDdBC15MGkMsMRL9tXzGZkC_RBbaCkWpGcJNcALNhBJXRpAo2-yzizLGd-9yadqHJGr39fK0YMNtTGaOFOgP1FR0X6QzzmkUe00b3L9LZTKP9Bx5k5o9aB8e9Yt5P4vR6OBZC2HydtE2PX_J1CU0BhhibgN1633SOwrM_5O7JnQFS0HyrhsVRbERAKlcmF7Trm4B3KMw4H-Fuwy9FpEuYo7_rGybNIHLK_UNlbg0QDx7eD9Ly3YT9D8F25Z_DdhAqJaPyaLF779UAb8ArIyfDNK_Pda4qc7GYsADPg69yvfFOxPpro8PhVYrCwjAucNFp--_NszKvA6EcHDYYzIbGpr1UXJFlJ0Yl4RYqHQEy8ulaV9zKSko_RHTGIC_iC-xhY3IiDr766eb2bhkgPJaADnq0rc-bvH8oH7nr09DLxCJH9LqAHjohADHTLD1pbBlYLtZRiQMmkD7ga9y1eibBHHQO2UdduXeiVkZsiuGd6WVGBoQUVDXRc4uH2aHt90LGbBLBiqksFXIFBpiD9puqE1cqvafc1AlH76_NHr2kq0NoRuU-6D1n3UThtpEELDgLSBD_E63XvyJGHWBC2vpA3xo3p7_cf6q1-HEtGle3I7PBBk5aiMZEGa0Zas1tBINjBijxQi9Lap5DrY2aH-gGD6WaWZKvUlK2Ha7fqZYLQ0cf-IPtf7HcfNLF7gnjaBRtnkwmazes5HSJrPhkEf17URf0wZzjWksKmotTT6w9X1zNWX7V_FCH3fmLdMkNx3HLCHaUZODD-zXZ6OZJRegUIcwBYzswwnjrbn4ZO8TAGLNSFb-MrDqAQeM7G38kgnSwlam6oJs_Pko_5nnKKIzFjrxjJcJURZFRh1I-0JytTki8Ca-zhYJ5JoZmRuuYoZApzy083-wqAbO_gQjNrKLVV6Cjlmsn8CxDrlHewfYAwit3T-t3wAtiuB-PWQnJuhf9UIiqjTYjetaN42yGFsdVphI5_XusLKyFbNW_t5lu6mM-bPw5c2mtYOGUFmDmnTJveBLRr6Gg5CmdgqFYtEWlCHxXzBWcLPqMmiGctUP2Ive0tb91bBKWzPIcu-SMOdz0tAqmm17ZB65DOE8C0J9IqJ0JvKJkODExLNjBHz8ggnXHzbDrhBGRtB9JNcjWdM9X219S_eGZXUNZ9x4bgHxwAWhYyy86DpWorSX7nER0-MC3eG285VHuPxZnGvlvs-4izBLuTVILSHA0pXH_WXxt3hsg1CqgPfEeYVGAHya1JJQ2LsTMVOYPyjB38VbcpLCWC-NmyLMCl9hWUiB5SqozFGcEpcFKtUMFLQO0ZhrgToL3v9xC9oGc40_4UFcduNWtjDOjM1zvtVaRrQpwCJBQo5A3HQXylAJx_PusORBKSHRehhvSCkkrZX5DE2lT26A2vrtHG-nbrFmSkshvFaI-B8SwiKiEasENjTvVBLcba70gu2jrZ86q8-pCu4drU9iJUqcKl3LKAPj08O8_7zIwmzcQmfkNecG3mKxicdj36eD9-TAP2atTCMGL7nfOQYmPJQeyz_rY7p7ITx2y_GgY8kM7kyY1wmHtYmWB8PODYRsVVlY4PvjlymlEJDQzKEqAWVA3XscPvC_oFIeVjwZlmX3kQGN0fHahLsMwRUU8wFAQdzWsLnIRJmvfRQvNTrEra_ZpvBi3Y-a7wWFBI9aNrn8uwvIZ2R8W9mrTFTFYdAiLDr5t0KmpDKF6Ye7KtXG9PcwKsjkQRLe9aOxAML6Mcop3ev_sysEBsxfEVURp281io3LZXzgp-Hn97XEfANxHuUQDDkhe1EH8lJlXLqz7lF8N4501v_gTtNg4_5eIH7cyR5edd98q87D6_R5NIA3JqSto632F16STBnidHSuBI6o9ir5nZdgQZCxY6dN1tuQmwdE6Y-L-HWM6OsBSoHqe_rMS_x_KMth5oj810_Lph1hYYbBW42-QRwT6k6Ci087SAkHHteIQR2wIbjwZg3rDBmb9VXI5nfLHA1JV9ewCZJ9oy9yyT-ZO3VN-f1LHOlGa1x-xXF1L52pOKh9SHq1TJlYQginsDuLwLDTSzqN4r7rtn2P10W61MgJcFZVhGKcZrDisE_LX6xklWSkSeYyNGXiRpZIkhw-Sb203oEYJj6PaQmSLAr-kHCNrT30QnRqP_XNR_k8rKMrCwlYx4K5ANXDuN3zvsRzth3run79AB93IitfvClrd_W-48qDLsI0sdHNB1n4lrKsVHZQ3ZlHKnLHWaaWKt64RVThXxUjvJTrQa3z-NDaGs-l9nESUCjqUUPAiW0s9uBAfplJs6CCc_KW8WSX67bF-OOqokZf82X9UFNcmtfjTMDINVmMJBXMKh3m2DEALC7PQcfhK8Tgp2hvsXlTaKxrpiP-Vvc0COlHPxaZcf18PGYL1UUiqOGNk8sQXERjSbNpDkbZMICg2HKMo7JvH-dV4eg3mJUYHVyOL_lLk4HOnq88GKNE6vrE7XjFn91VPpMU21wyniqiGTQMtmaZUA3oOSZS0-g3mVCqpkyc9jQUSbtmXKYssd5tAXcI65L_izeuyj1lm2bHC6pbUHiWtew1_76RNqirRo2V5uwRPiEQyg8DafBGyM6EUndYe8yRtjU-6MknXMPTWScWZthnnhjq0Cg7EV5yXRZTl11onpqkKb8VK1rtnt32FABowRw976ernkh4y-PSjMiUXeFQXIjfzymW0M_SUuq44ZKHxDdwXS2xjev1PQX1P2QlwGQNPwv5PiM=', 'id': 'rs_68f6b220dd3481909bd1d7423ee7e3f6', 'format': 'openai-responses-v1', 'index': 0}])\n",
            "ChatCompletionMessage(content='', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageFunctionToolCall(id='call_9aQoiXx7oP6pqkJNpla5YW5I', function=Function(arguments='{\"query\":\"EU AI Act generative AI 2024 2025 overview\", \"max_results\": 3}', name='tavily_search'), type='function', index=0)], reasoning='**Gathering context for the creative brief**\\n\\nI need to create a JSON \"Creative Brief\" with specific fields like target audience, primary goal, and key takeaway. The developerâ€™s instruction mentions searching for context if the userâ€™s input is lacking details. The request was about \"the futur of generative AI,\" which seems ambiguous and suggests I should perform web searches to gather necessary context. Iâ€™ve already run some searches, but I need to ensure all aspects are covered before finalizing anything.**Searching for additional sources**\\n\\nI need to gather a few more sources related to the EU AI Act and US regulatory proposals, along with copyright cases like Getty and Shutterstock lawsuits. It\\'s essential to check if all aspects of the topic are covered, and if not, I should perform additional searches. I think our previous sources covered most areas, but we could benefit from more about copyright cases and watermarking standards. So, Iâ€™ll use functions.tavily_search to search for relevant topics.', reasoning_details=[{'type': 'reasoning.summary', 'summary': '**Gathering context for the creative brief**\\n\\nI need to create a JSON \"Creative Brief\" with specific fields like target audience, primary goal, and key takeaway. The developerâ€™s instruction mentions searching for context if the userâ€™s input is lacking details. The request was about \"the futur of generative AI,\" which seems ambiguous and suggests I should perform web searches to gather necessary context. Iâ€™ve already run some searches, but I need to ensure all aspects are covered before finalizing anything.**Searching for additional sources**\\n\\nI need to gather a few more sources related to the EU AI Act and US regulatory proposals, along with copyright cases like Getty and Shutterstock lawsuits. It\\'s essential to check if all aspects of the topic are covered, and if not, I should perform additional searches. I think our previous sources covered most areas, but we could benefit from more about copyright cases and watermarking standards. So, Iâ€™ll use functions.tavily_search to search for relevant topics.', 'format': 'openai-responses-v1', 'index': 0}, {'type': 'reasoning.encrypted', 'data': 'gAAAAABo9rJVnYxt1zce3yyHQgph7R9DHfg8GmoEAT0p_zyCIE-H4RB3NBSqY_pDkzt-BUhD-qkmcpXHj085NlxHMMv6l9Icxd5PkeBiQ51bHsLmHmJqBK4IqdiqZNuhNp4zrbrumPPVxAiqX39Ytkzes7D0yimAxZunzCm6ivbEMh_uDH8LY8wWmPhW4hjUa-tChoMqra0q3ds0RpQKSTs7A80u6P2t0POot0dASNFCk3_lLTMngcN4R6YLORNTgDCr477-hSlN-THHcWykDCfoUVpnV54cFS_zzYbgi32NfNi-JdNYKsgSnu5Q7rZJQyxQpbgf-68CU-0LTP7jxLXSwi6YxupVi2G2W3Cyd2z7jHsYUdAQeQ1aQZqmhN9bq-rZMnySnk60NfoBIa4wp4sEeCzv4YFMX9lVOOgbQi5Yr3iDLC3N_4-rD0cfJ1oow8j-MASEk2PvPAjmv4MxEdR6Q3OjMLdCSxmMCum0fpOBmLqyh2vWb4d60-Ko_H_YPkfUhBNo8w50MC3KyZkNoHa0foivGUR6NHTEsvXEks5JaGGB1o7QIdcdOKVduSiDSnzefc9cfGRgS_LDlq53PB8nQ9Le4thj3X1FKYTGfowzuVzWvDyZlV7XETacNQhBMrXp1OC-ZdTlP7ag0aZYquwWaXKsB3f6nAac4dSeGRIvYFeibNJMEUczQPZrgCgndTyiioWJXsz1Re4bboGAdx6StfgpZuwcIIzq3CNlDPV73-QhZ3pdO3BN7xyiIWJxH1Cw5LvVZW9b_C8kft9bg1R5Y89GdLLWlVKVqR-UKOxf1fBSOyyQ5A1ohcJ7eBFiP4RxOb9dtHIobKNLJ-oRFm47b9sTe90_vU7KqCC9KPIRTsHawOCjCNm8xhVW7Xhy-QSw4oiD9hUQyK8nmD4NNgumRqL-JxyGh-Xs_2m6GG6jk1VZ1ZZrSFpHdEhEbPel4ZqjVksdWj0Foe9307PeVv6lwPYD_BxcXrfhJ1JZzk55Csmo6Boj-draTwYbC_ZImUotJ1BatdLUM6Qjd9_SW6BJpVfP-BJuUTp2KD4VDpSnD0aeCL79hhHk28k9g0L1UQj1tzvNbvKdkNcFlB4QdUSPOKyL12b6vjDvMCSpJ_BnyU4EbscP1y0Zz2QIu6zvRdffgtzQPrJ3--4-lE4h7cqYDZcLdoxRrC1L5lRP_zAV0FUcw2HUd987kho4_UZ67_v6x7M8ylwyqphvHbA7HssWGiYJ6Mfq7M3M3wYhwtNARqPi9ditTemk81o8dk_tdgGBAlWh78V1v2r1dKN5tMaZIExwxOzqvfUNXnH0fsZOmrWtDDmSPMUDUvsTLY__OMH6LwnCl_KnBjb08z1Cu9sbCi-DEjOZd4Kc_FsOrzIfY6oggyo6LCCjni6Hrq7oHSnSLxRDd7gIShYeLuTQNEcPqJoPI2Zqnik7bsCBH9NojIqgZSaej5v8WeLNaRAU7owBx4C873AAtz7Q3_AcZ_bA23pDhNGlnmdpfDblpJOJBdN99f8rVQigEQH4h1Cw0Eopm8OD3uDQhJrRYBc8Ab0hrtKESd-EZ7H1FaG4tGhgJgbb9EMnaXrjH_lSJpE7lEBK93Jh-5HSc8cdKpKykEc9Rgl_bxtEZYhXUxlc16Mcu4Qn27xRvus5UJerNXGFxOroe0RHE8NnOFtz2xQR2pfBbfiGgd6bwj96ZBHmv6iP3LZA0BUiu8LbhnpcF_SKBdsyUoa1lk3JQN9yVkKKzkpLCkWrUgRJiwqSR4ZMWDFJ10bVywqtB5LM1phIBXtnhQwlEJ0nOr7SRtdH_I9xCWtCGQKN4s62B5-kQSjxBwQ_iFpVwZauj2U_YxcjNWwTCSy0jzUJMJMTmlbok4j0Ia5rDIvWZG5E0XpFaeRcFuyn2vEwQLlp3zfsGkpsK0ldrfwF3Qh_tJuaECP6PXC4WduPLeZTbY3TFB9tNxfUEUzgl7FwH40GC0Z6UKSpKLD76QfSFn_2cit3iqH_j4J1YKyIkdF3q0LRrkqBSexviAIZc4o8EEYry0eALEVq7fTjJeDBbAlGJFzvVehbws41IRKIbTzCgCAWrhP1oUt4ZyZ4f5tDoxRqKG0N5TF8Z5iFl1JoFVTJQeYVhfNNEfqH1mP7yE8LkF4wxBFk2m2ticjzZHw9F4IEOZK6CVu1xUldyLGHj72pvdfGS25C4INvtPJiGq4wUhC6C6-m80byb7Je3la-XzFSDNacttW8eyZWCPx0mOKvcp257357HtVdL4x63_lGu3FyzgIH6HzNGaN-r9bWD5HA6diaF_MT48f_edV03xJIKYfDDj3mByWtVNpkHpqJoFl6Tg_gLadt-ohoQJ2KAPa7PgFMVhJ2zFDpcFguz9dMfZdTZyjNdbRUOeJllW6PfnjuwzcteSWZsDZ9pG0oxMsUXH6ZOKi1CFKbVYpuk2agRo_vH-3QEGQ7sOElOT35TDVMMkchxI-qclfZCGTuFnSh-jx9OG8yyyG-_AGxJZy5wOpxFhg7PyAiNFVljha6HtUITtbdYO11vXVZrWawWtD2NT_BVZIfMrpw4qyYovOYs7HqbGrtTddSVTTj7tZ7RmrtgbR0JIMYcqihDqBsqC-YaaEs-JoNwtA5-JHhbajsoC6XJJo3iUWqdFHiuaiPvxoXnkbEkeCsGETqCsaRaB-XrjDdhITDvfC16kwOmyOMfYvX4ayUlLq3JSlJHfz7kTwZ8vMW3Nyd8FJN5uFxkpVcOmnz6F95SccYjA6RRIy-FpideV87IGIAkejc_edKxCM0Ar1QHXcPjDe6qgihkKWV4AfoCdnNKdivdI_Vj67IVT63Z13xftkb-0aD9uCIX7T5PuJmxtwG7Nmtr8NgFZulybfMYHwSmWDezWlbmGiIS0nMkmcZH3XlM0kIUjnpluBWgxiU-UmCYTTyMdNNThxASd4ONzziV9ljvkGYiPdnWO-OPwP7nvKfQR3E6PnqRbdNfaNWuxvxdM9JDkbf8UQoXy1YByNJxsaK3OduznfIEPxQh1fT-1zxYDKHfU8oPQ6cdR_dxn4GTtCca-5QHlE-VASKhsIx5xue-Q0jx_a7QETAeSnogNycdJl9x2zScprfAZzvl4wVYRqWcqffL0RojkzTSpo2uWWXiZrMRh8o627GmL75Gp3e7dy2WM7GVtPz6K_7tLPTTHcP0tSq-xcIqjv1rpuchykRSHho52Ot1cWTiMH_FXXlfcDU-0E5_BiljHU3BFs68qvE7EYV74dHzKNT7TE7VF5Fuje0DbRHtjWVVmB4cpbRBTRvMI-FIkwLpQFYcEfNpcV_2I0YD-ZkXVcOJ0WPxgj7qA_2B_u2hNXzPPYbiYDM-pXUB3t7lCWkDaFNoB9XipFZaKmM9tJ3l8r7jtjCoCo_ygzYi0hInb9nBJu3KDx4oVKLZk3WFTlcUP-HNwhewyXsgPJc9fspaHhFxh71H1w7JR5IM-xMJuG5GTuoOMpY8Z41L8GmuGofGRm5pH99p7HM9kabopzln9-TiOigIqxw3CRECOng-20IenxTEXG1qs23BJKgTBLEsCF8gO5PES9CWsub4EVLOAbLTkGJwCvFLUQWvKtSqiv8AUwpabpS5_8EJ-_Va_UHSyzZEOsYiFWTcTYBwmUxnx2WCChBhPpsTEB4eLdQbDsxfwh1ibE4hAJNy8OA-AT-2Jt8Bx_4gbyhaxBTn0Q6LeSt1NMtzjyG1Ge2ZVDPKCbCkH3bt4z9g-8AdTQKrHM8mK6OrrZ2VkkavYsevzEJxIoJdbt9YfwoyQGbFtRdrgw1zFG8bYQPtuAwJ4--ERwp6scU_lCW8HpgKmvL6wSXbMU8E_gtB0INcCGiCh-qodVs1F6aK6NIUhT4FAoj2TkpH8QNgpflM02R9sxTWgZCIOevg-lvO8OOvZtcCofMZpDTY8JT_r0vOw_IQ2oF3vT5LrHsJljBEu33WzVw9DSMiFqOwQ7v2VF5H_0uoDLPK3U_KHRI-EKVzk-gqFa3HKyifD0z1jN9xoiaVILZMxXBiLeh0YoKbOAznGeO01-L3tGW-tOKNT6cbaYexKh1IABIXf5SCfAPihZZl5tyFwhbxdqTD6Uvdi5WYSs54eOxSPb46DCdDcu14fIcmtNXrFCUjx_GtBZkqKN1wSMbLWd6PJzcoM1GPPXA2CkPlJN-HQgd2n_IeCaeZRljT2lYs44gbfV4ZD_HWxbFAG9G3znlwGGfhtx84ttcclBetzwsFXVtAOtyvldnuJPWk9xMwjGSi670FwsQ1ogSAZ1ZOyUSM01z_T53ykDdRoNx2kR85hk63eUQmAQBCvTsgPTx04t_RZETrbdNvQ9M0VbAOl17GVfBdV3N2GWeO_BqLQN2KWaK241yj70EoiW64jIGpg6h-HiJ87_UBTR_pRkNnJZ1i3TxT1OtM1m4r_OqroHBuqXnNRacO9OCv0sTeLsHxZyoT-r6RsNzzseJvUZwpPH3tsaQ2GnlXkLsRoaIzrTsVmax4WMEZmI2EwXLpZRiuZRluN50AyQ5xK9rCPKHC6QH446qUeyuxXXqWwHPeU_kM1BCi4b_mh_jMMDS-07l8pwavKAUVMO1BvabJoHvdX5yTJv3g1TbBdQQOAkk-lefrBDa7x5bZUIwAKIKrr6m8hy60zm9ciwJU248GKMnoifXQ3H4e0D-Az4T_JgoDKJCBJURXa_iUIMtf70jOV0oMGAPOy3Oiu9e2vuh37IUjwI19DF_uAL-y1WDFmH3yJ2gX0GeiRlzrL3VZCuOS9jERBbGHtAScYXwHzp5uINaBrK7jJs3TIwgAREmTmZk4KTXt-TSIaK81EQ_lNfDfYowkWGg6ALU351BR74XuRxrhhwl22PTwYT71UziXMI_yHQ6Bka9M8yCDroW526WuysTBNRXyXirCFA1hcCvhkaEyOxcBzS5kShLvCB9sY0gDaTdbE6IZca6jtNfWPvDJy3fCwCCmGW-CoFJD2cIr-h8ctFb33c=', 'id': 'rs_68f6b249ab4c8190968dcb4c738e30a9', 'format': 'openai-responses-v1', 'index': 0}])\n",
            "iteration : 2\n",
            "ChatCompletionMessage(content='', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageFunctionToolCall(id='call_9aQoiXx7oP6pqkJNpla5YW5I', function=Function(arguments='{\"query\":\"EU AI Act generative AI 2024 2025 overview\", \"max_results\": 3}', name='tavily_search'), type='function', index=0)], reasoning='**Gathering context for the creative brief**\\n\\nI need to create a JSON \"Creative Brief\" with specific fields like target audience, primary goal, and key takeaway. The developerâ€™s instruction mentions searching for context if the userâ€™s input is lacking details. The request was about \"the futur of generative AI,\" which seems ambiguous and suggests I should perform web searches to gather necessary context. Iâ€™ve already run some searches, but I need to ensure all aspects are covered before finalizing anything.**Searching for additional sources**\\n\\nI need to gather a few more sources related to the EU AI Act and US regulatory proposals, along with copyright cases like Getty and Shutterstock lawsuits. It\\'s essential to check if all aspects of the topic are covered, and if not, I should perform additional searches. I think our previous sources covered most areas, but we could benefit from more about copyright cases and watermarking standards. So, Iâ€™ll use functions.tavily_search to search for relevant topics.', reasoning_details=[{'type': 'reasoning.summary', 'summary': '**Gathering context for the creative brief**\\n\\nI need to create a JSON \"Creative Brief\" with specific fields like target audience, primary goal, and key takeaway. The developerâ€™s instruction mentions searching for context if the userâ€™s input is lacking details. The request was about \"the futur of generative AI,\" which seems ambiguous and suggests I should perform web searches to gather necessary context. Iâ€™ve already run some searches, but I need to ensure all aspects are covered before finalizing anything.**Searching for additional sources**\\n\\nI need to gather a few more sources related to the EU AI Act and US regulatory proposals, along with copyright cases like Getty and Shutterstock lawsuits. It\\'s essential to check if all aspects of the topic are covered, and if not, I should perform additional searches. I think our previous sources covered most areas, but we could benefit from more about copyright cases and watermarking standards. So, Iâ€™ll use functions.tavily_search to search for relevant topics.', 'format': 'openai-responses-v1', 'index': 0}, {'type': 'reasoning.encrypted', 'data': 'gAAAAABo9rJVnYxt1zce3yyHQgph7R9DHfg8GmoEAT0p_zyCIE-H4RB3NBSqY_pDkzt-BUhD-qkmcpXHj085NlxHMMv6l9Icxd5PkeBiQ51bHsLmHmJqBK4IqdiqZNuhNp4zrbrumPPVxAiqX39Ytkzes7D0yimAxZunzCm6ivbEMh_uDH8LY8wWmPhW4hjUa-tChoMqra0q3ds0RpQKSTs7A80u6P2t0POot0dASNFCk3_lLTMngcN4R6YLORNTgDCr477-hSlN-THHcWykDCfoUVpnV54cFS_zzYbgi32NfNi-JdNYKsgSnu5Q7rZJQyxQpbgf-68CU-0LTP7jxLXSwi6YxupVi2G2W3Cyd2z7jHsYUdAQeQ1aQZqmhN9bq-rZMnySnk60NfoBIa4wp4sEeCzv4YFMX9lVOOgbQi5Yr3iDLC3N_4-rD0cfJ1oow8j-MASEk2PvPAjmv4MxEdR6Q3OjMLdCSxmMCum0fpOBmLqyh2vWb4d60-Ko_H_YPkfUhBNo8w50MC3KyZkNoHa0foivGUR6NHTEsvXEks5JaGGB1o7QIdcdOKVduSiDSnzefc9cfGRgS_LDlq53PB8nQ9Le4thj3X1FKYTGfowzuVzWvDyZlV7XETacNQhBMrXp1OC-ZdTlP7ag0aZYquwWaXKsB3f6nAac4dSeGRIvYFeibNJMEUczQPZrgCgndTyiioWJXsz1Re4bboGAdx6StfgpZuwcIIzq3CNlDPV73-QhZ3pdO3BN7xyiIWJxH1Cw5LvVZW9b_C8kft9bg1R5Y89GdLLWlVKVqR-UKOxf1fBSOyyQ5A1ohcJ7eBFiP4RxOb9dtHIobKNLJ-oRFm47b9sTe90_vU7KqCC9KPIRTsHawOCjCNm8xhVW7Xhy-QSw4oiD9hUQyK8nmD4NNgumRqL-JxyGh-Xs_2m6GG6jk1VZ1ZZrSFpHdEhEbPel4ZqjVksdWj0Foe9307PeVv6lwPYD_BxcXrfhJ1JZzk55Csmo6Boj-draTwYbC_ZImUotJ1BatdLUM6Qjd9_SW6BJpVfP-BJuUTp2KD4VDpSnD0aeCL79hhHk28k9g0L1UQj1tzvNbvKdkNcFlB4QdUSPOKyL12b6vjDvMCSpJ_BnyU4EbscP1y0Zz2QIu6zvRdffgtzQPrJ3--4-lE4h7cqYDZcLdoxRrC1L5lRP_zAV0FUcw2HUd987kho4_UZ67_v6x7M8ylwyqphvHbA7HssWGiYJ6Mfq7M3M3wYhwtNARqPi9ditTemk81o8dk_tdgGBAlWh78V1v2r1dKN5tMaZIExwxOzqvfUNXnH0fsZOmrWtDDmSPMUDUvsTLY__OMH6LwnCl_KnBjb08z1Cu9sbCi-DEjOZd4Kc_FsOrzIfY6oggyo6LCCjni6Hrq7oHSnSLxRDd7gIShYeLuTQNEcPqJoPI2Zqnik7bsCBH9NojIqgZSaej5v8WeLNaRAU7owBx4C873AAtz7Q3_AcZ_bA23pDhNGlnmdpfDblpJOJBdN99f8rVQigEQH4h1Cw0Eopm8OD3uDQhJrRYBc8Ab0hrtKESd-EZ7H1FaG4tGhgJgbb9EMnaXrjH_lSJpE7lEBK93Jh-5HSc8cdKpKykEc9Rgl_bxtEZYhXUxlc16Mcu4Qn27xRvus5UJerNXGFxOroe0RHE8NnOFtz2xQR2pfBbfiGgd6bwj96ZBHmv6iP3LZA0BUiu8LbhnpcF_SKBdsyUoa1lk3JQN9yVkKKzkpLCkWrUgRJiwqSR4ZMWDFJ10bVywqtB5LM1phIBXtnhQwlEJ0nOr7SRtdH_I9xCWtCGQKN4s62B5-kQSjxBwQ_iFpVwZauj2U_YxcjNWwTCSy0jzUJMJMTmlbok4j0Ia5rDIvWZG5E0XpFaeRcFuyn2vEwQLlp3zfsGkpsK0ldrfwF3Qh_tJuaECP6PXC4WduPLeZTbY3TFB9tNxfUEUzgl7FwH40GC0Z6UKSpKLD76QfSFn_2cit3iqH_j4J1YKyIkdF3q0LRrkqBSexviAIZc4o8EEYry0eALEVq7fTjJeDBbAlGJFzvVehbws41IRKIbTzCgCAWrhP1oUt4ZyZ4f5tDoxRqKG0N5TF8Z5iFl1JoFVTJQeYVhfNNEfqH1mP7yE8LkF4wxBFk2m2ticjzZHw9F4IEOZK6CVu1xUldyLGHj72pvdfGS25C4INvtPJiGq4wUhC6C6-m80byb7Je3la-XzFSDNacttW8eyZWCPx0mOKvcp257357HtVdL4x63_lGu3FyzgIH6HzNGaN-r9bWD5HA6diaF_MT48f_edV03xJIKYfDDj3mByWtVNpkHpqJoFl6Tg_gLadt-ohoQJ2KAPa7PgFMVhJ2zFDpcFguz9dMfZdTZyjNdbRUOeJllW6PfnjuwzcteSWZsDZ9pG0oxMsUXH6ZOKi1CFKbVYpuk2agRo_vH-3QEGQ7sOElOT35TDVMMkchxI-qclfZCGTuFnSh-jx9OG8yyyG-_AGxJZy5wOpxFhg7PyAiNFVljha6HtUITtbdYO11vXVZrWawWtD2NT_BVZIfMrpw4qyYovOYs7HqbGrtTddSVTTj7tZ7RmrtgbR0JIMYcqihDqBsqC-YaaEs-JoNwtA5-JHhbajsoC6XJJo3iUWqdFHiuaiPvxoXnkbEkeCsGETqCsaRaB-XrjDdhITDvfC16kwOmyOMfYvX4ayUlLq3JSlJHfz7kTwZ8vMW3Nyd8FJN5uFxkpVcOmnz6F95SccYjA6RRIy-FpideV87IGIAkejc_edKxCM0Ar1QHXcPjDe6qgihkKWV4AfoCdnNKdivdI_Vj67IVT63Z13xftkb-0aD9uCIX7T5PuJmxtwG7Nmtr8NgFZulybfMYHwSmWDezWlbmGiIS0nMkmcZH3XlM0kIUjnpluBWgxiU-UmCYTTyMdNNThxASd4ONzziV9ljvkGYiPdnWO-OPwP7nvKfQR3E6PnqRbdNfaNWuxvxdM9JDkbf8UQoXy1YByNJxsaK3OduznfIEPxQh1fT-1zxYDKHfU8oPQ6cdR_dxn4GTtCca-5QHlE-VASKhsIx5xue-Q0jx_a7QETAeSnogNycdJl9x2zScprfAZzvl4wVYRqWcqffL0RojkzTSpo2uWWXiZrMRh8o627GmL75Gp3e7dy2WM7GVtPz6K_7tLPTTHcP0tSq-xcIqjv1rpuchykRSHho52Ot1cWTiMH_FXXlfcDU-0E5_BiljHU3BFs68qvE7EYV74dHzKNT7TE7VF5Fuje0DbRHtjWVVmB4cpbRBTRvMI-FIkwLpQFYcEfNpcV_2I0YD-ZkXVcOJ0WPxgj7qA_2B_u2hNXzPPYbiYDM-pXUB3t7lCWkDaFNoB9XipFZaKmM9tJ3l8r7jtjCoCo_ygzYi0hInb9nBJu3KDx4oVKLZk3WFTlcUP-HNwhewyXsgPJc9fspaHhFxh71H1w7JR5IM-xMJuG5GTuoOMpY8Z41L8GmuGofGRm5pH99p7HM9kabopzln9-TiOigIqxw3CRECOng-20IenxTEXG1qs23BJKgTBLEsCF8gO5PES9CWsub4EVLOAbLTkGJwCvFLUQWvKtSqiv8AUwpabpS5_8EJ-_Va_UHSyzZEOsYiFWTcTYBwmUxnx2WCChBhPpsTEB4eLdQbDsxfwh1ibE4hAJNy8OA-AT-2Jt8Bx_4gbyhaxBTn0Q6LeSt1NMtzjyG1Ge2ZVDPKCbCkH3bt4z9g-8AdTQKrHM8mK6OrrZ2VkkavYsevzEJxIoJdbt9YfwoyQGbFtRdrgw1zFG8bYQPtuAwJ4--ERwp6scU_lCW8HpgKmvL6wSXbMU8E_gtB0INcCGiCh-qodVs1F6aK6NIUhT4FAoj2TkpH8QNgpflM02R9sxTWgZCIOevg-lvO8OOvZtcCofMZpDTY8JT_r0vOw_IQ2oF3vT5LrHsJljBEu33WzVw9DSMiFqOwQ7v2VF5H_0uoDLPK3U_KHRI-EKVzk-gqFa3HKyifD0z1jN9xoiaVILZMxXBiLeh0YoKbOAznGeO01-L3tGW-tOKNT6cbaYexKh1IABIXf5SCfAPihZZl5tyFwhbxdqTD6Uvdi5WYSs54eOxSPb46DCdDcu14fIcmtNXrFCUjx_GtBZkqKN1wSMbLWd6PJzcoM1GPPXA2CkPlJN-HQgd2n_IeCaeZRljT2lYs44gbfV4ZD_HWxbFAG9G3znlwGGfhtx84ttcclBetzwsFXVtAOtyvldnuJPWk9xMwjGSi670FwsQ1ogSAZ1ZOyUSM01z_T53ykDdRoNx2kR85hk63eUQmAQBCvTsgPTx04t_RZETrbdNvQ9M0VbAOl17GVfBdV3N2GWeO_BqLQN2KWaK241yj70EoiW64jIGpg6h-HiJ87_UBTR_pRkNnJZ1i3TxT1OtM1m4r_OqroHBuqXnNRacO9OCv0sTeLsHxZyoT-r6RsNzzseJvUZwpPH3tsaQ2GnlXkLsRoaIzrTsVmax4WMEZmI2EwXLpZRiuZRluN50AyQ5xK9rCPKHC6QH446qUeyuxXXqWwHPeU_kM1BCi4b_mh_jMMDS-07l8pwavKAUVMO1BvabJoHvdX5yTJv3g1TbBdQQOAkk-lefrBDa7x5bZUIwAKIKrr6m8hy60zm9ciwJU248GKMnoifXQ3H4e0D-Az4T_JgoDKJCBJURXa_iUIMtf70jOV0oMGAPOy3Oiu9e2vuh37IUjwI19DF_uAL-y1WDFmH3yJ2gX0GeiRlzrL3VZCuOS9jERBbGHtAScYXwHzp5uINaBrK7jJs3TIwgAREmTmZk4KTXt-TSIaK81EQ_lNfDfYowkWGg6ALU351BR74XuRxrhhwl22PTwYT71UziXMI_yHQ6Bka9M8yCDroW526WuysTBNRXyXirCFA1hcCvhkaEyOxcBzS5kShLvCB9sY0gDaTdbE6IZca6jtNfWPvDJy3fCwCCmGW-CoFJD2cIr-h8ctFb33c=', 'id': 'rs_68f6b249ab4c8190968dcb4c738e30a9', 'format': 'openai-responses-v1', 'index': 0}])\n",
            "ChatCompletionMessage(content='', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageFunctionToolCall(id='call_VfEM7aJB1t88WT781vlu9DkF', function=Function(arguments='{\"query\":\"AI compute chips NVIDIA 2025 trends GPUs AI accelerators 2025 semiconductors AI energy efficiency\",\"max_results\":3}', name='tavily_search'), type='function', index=0), ChatCompletionMessageFunctionToolCall(id='call_UDFZUJTWLJ87LlxYNX9RPMVM', function=Function(arguments='{\"query\":\"watermarking provenance C2PA generative AI detection watermarking 2024 2025 standards\",\"max_results\":3}', name='tavily_search'), type='function', index=1), ChatCompletionMessageFunctionToolCall(id='call_ckUqX3wVkMsCyt4AdGcIqPGK', function=Function(arguments='{\"query\":\"copyright lawsuits generative AI training data 2023 2024 2025 legal cases authors lawsuit OpenAI Getty Images Stability 2023 2024 2025\",\"max_results\":3}', name='tavily_search'), type='function', index=2), ChatCompletionMessageFunctionToolCall(id='call_j2f8n1THxUBNive3bvAVfzyC', function=Function(arguments='{\"query\":\"workforce reskilling generative AI jobs automation reskilling programs 2025 PwC report reskilling\",\"max_results\":3}', name='tavily_search'), type='function', index=3), ChatCompletionMessageFunctionToolCall(id='call_ZCGUYR2Oxs3oB6QTGmXBnDSu', function=Function(arguments='{\"query\":\"AI governance standards ISO IEEE OECD AI principles 2025 guidance generative AI standards\",\"max_results\":3}', name='tavily_search'), type='function', index=4)], reasoning=\"**Producing JSON brief**\\n\\nI need to create a JSON creative brief based on developer instructions. It must include details like target audience, primary goal, professional tone, key takeaway, and research context. The developer's message also mentioned a list of letters about gathering context if user input is lacking, but I've already done that through web searches. Now, I have to ensure all topics are covered. If not, Iâ€™ll do more searches and summarize findings in the research context.**Creating a generative AI presentation**\\n\\nThe user has asked for a comprehensive presentation on the future of generative AI, which is a broad request covering various aspects like technology, applications across industries, economic impact, ethics, and more.\\n\\nThe search tool provided many relevant topics, such as models, applications, productivity, and trends from reputable sources. However, it seems I might need to include some hardware details, as well as topics like data governance, copyright issues, education, and international competition to ensure a well-rounded presentation.**Conducting targeted searches**\\n\\nI need to carry out some focused searches to fill in gaps for the presentation. Iâ€™ll look into topics like AI compute semiconductors, watermarking and provenance with C2PA, data licensing and copyright lawsuits related to generative AI, adversarial attacks, and workforce reskilling.\\n\\nIt seems wise to use the functions.tavily_search tool to tackle these gaps effectively. I might even consider running multiple searches at once with multi_tool_use.parallel to get results from different queries, aiming for around three results each.\", reasoning_details=[{'type': 'reasoning.summary', 'summary': \"**Producing JSON brief**\\n\\nI need to create a JSON creative brief based on developer instructions. It must include details like target audience, primary goal, professional tone, key takeaway, and research context. The developer's message also mentioned a list of letters about gathering context if user input is lacking, but I've already done that through web searches. Now, I have to ensure all topics are covered. If not, Iâ€™ll do more searches and summarize findings in the research context.**Creating a generative AI presentation**\\n\\nThe user has asked for a comprehensive presentation on the future of generative AI, which is a broad request covering various aspects like technology, applications across industries, economic impact, ethics, and more.\\n\\nThe search tool provided many relevant topics, such as models, applications, productivity, and trends from reputable sources. However, it seems I might need to include some hardware details, as well as topics like data governance, copyright issues, education, and international competition to ensure a well-rounded presentation.**Conducting targeted searches**\\n\\nI need to carry out some focused searches to fill in gaps for the presentation. Iâ€™ll look into topics like AI compute semiconductors, watermarking and provenance with C2PA, data licensing and copyright lawsuits related to generative AI, adversarial attacks, and workforce reskilling.\\n\\nIt seems wise to use the functions.tavily_search tool to tackle these gaps effectively. I might even consider running multiple searches at once with multi_tool_use.parallel to get results from different queries, aiming for around three results each.\", 'format': 'openai-responses-v1', 'index': 0}, {'type': 'reasoning.encrypted', 'data': 'gAAAAABo9rJrpwedkFOgiJ51NSkrfCXgQR1PsSLIPWSdJkUkAulUxQRMFNXAuVATRTJJzlY-o-qrIi1l3zvLcJUL1LwUKH7AxvBPfbS5W5aMguxZ8JvcXWNQjgk9lCj3qiFs9iQmFdiNcdt8jMJ9ryxgcdr23LRMbMnb_W0t3zi0Atcb2_8ZkQUvxIJ6nhPsiMivRnauolItBmdH8aOImnPJESRd4OieZvxoXAqJShLRb1RDQS7bUKg87ULgFtuKz7SOzFuRjk9MSb3s1q58Dyc59fU0h399Pm_vcg8PpxS09VuwopX3FJsXaol4-enaLojKlxfDKCWojcaFT6Luye1yuTz6etHaImSiDUJgQSIcx4NIK6AvyElfLwOtDmd_t2E4Ur3ffv6QHvnEfukhd8b0KfMxJsolBcrwR_tpbrA1GrKyM_PQrzbsNJvbGRqzykeGD_vRrOuYjM5_yHXzugxaAitl27QblVif7j5EJBfnSe_f_IkxdZxR5JToTk_TGSmVFU5989RWCASdBy63YYFd3eW_uPPEMZYUqVyCg3XbKxFTGvCspJfSdTQJ7wLTzzgk4CD8rI50ZnamwfPpjRCRCAgJyPrGy3fFwqwlCzPwr9OGQFuN0sflQ2JM3HqNSd4QsvjbCu3ctfizkdPeaULi1jhhOTKfTp0aBom7C-Gooqp_wlEXETCfydOlywk8E28fXil8clLG9cCXec5sDW_RsP2UcMHCyNfnAO3eRw0BJsDA1o-yng3K9WJ6E6jfc2QYpSnPgV_oyzAIihTN1a6Er8teUy4ko_M0X0LZXObvoaU1z9CPW2mtL1H6bL_mqqlalDiQ9DTyRINWm_nQUrGZlxGYKkMC4QApkL0zONua8OJaWgl0lXTWTqghxZ3zE6ptVcu78ydV2jjj1j4suwiDZARi4u-k7ZG2xnDQqXVR3TLGFpeofnZjCGNB050WJTCYdU3EVVEj1CXPop7ODZ6BZ0VLPNhR9cUBhxQgWPOe1cR1KfjZ5krtf4Q98A-BMAYyQCWP1HQ-xB_hwQFx-HXIVyV0qnfuLT4nK6jd0GDTgy77oUMMbRhPAec0_cH0SVE4NWo_rqCaBNDV2liKOiPF_TLTnoYgQ45S_9eLYLBblwCNkqOYXQOmUTDBwpOejhrnLZOk2rl-VyEH9i_r_cEOgHE9GmeZA25ITfzGHI2nUyXtvNEcco8mnG3_nvvE2LAxC6zUuOLPSbGmW5henxu-HsL9CzyhHbJox5CiSFSOGLGWuYTZcNI2-aKOE5cw2IFwjcgu9ZkAHS03d6-YDgG2KWL0xb0Is47PDor1LFnLE-2ljz4s3LIzXhqJ23U6w85vkLsXiF3ELTWK7I7PmK2IsaKOwuGDGll_7hODRcMDukTmCM2-92_J6fA-vqb45EXpCdxX8ii06ZVoLhNkJAqo89kR4i1KVZWAbPYte5mUkMHEDkUIfrLgM4m7r8V6Bfq2w92n7Jy2P_eqFhHubCL_Z3BnfsKEpJLZQPnPE1Wbqdm4R6rcn0KM_HmjqceIyGWPatiBFrvQoR6KuN7Z1_rVetDL004j6N3HDyR2IHFN4Oz4_C-2g_4H_cJ9R4FdSQnoiIowMiL2lOemTSOM_JtpXwfH_PSgTOdFkbTLTsJAH4bwnbZ9ulhkAUgEE8Ahmnoy7EMG4IFSZW-Q75Av3LQz7rhGshR1MuLDxLmxY7vECq6NxPgV5BMxu4-oxX3rZJOV5IfQOizOjz-34CYO9grw26iV61YVC8rdwIuytmoJ6qNdIVHa5iS3uNFTI_F4qvh5dQtEZcbBvgHIODeY4FvoOYC6gUNDrJz8iRsHrHpkD-KxCyWS_QXv9KT0aM03aa53rJjpcAOClozFAzFfNcM45kbYBYHUfBKauFCVU76WmWC883HdYFqGWZ0Kg_yra9gi7hmhtJn5n7btJVpy8vflMY4dkkW5PI_uzG36tJSBeLIKm6ZrGwuQPxyksJeOluqUJ2zg_eYkJ9QayAoW1hYJs7rbNGc5f8S7Vjoy3S0koKMMGZz7DAcUXWTVi21qlmmWG69zdMIXwUhAdywVeCzrLDvGKxMBMj9wnaK-58A8HmKo_GyKdmN0hZVmtfEA_blnrWze6ibeImy_FnBsciEYIPBiSKoE2tXRt4J_YG--pURB4pMFyoIyT2LqWKd0mGjzcceYQS6v45MYt2ZZu7QtCzQULILbdItfbSTmzWR9qURsxEeegoQ4Tze-LDUKZn3IOCKFfHTW4BOd5h4X56tndxNHLj_jizoxXAI6JcrWy9XOe54BFB6o1rWbyOzPLpOjl5shw9ZaMTfAOb83wDQX-dgoI8fa81Zyx3QVxMsyr8_zvoKPlzhDFtEP-eBnHl-Z7VKHiodKvkYWbBpjGJqlqKUKrA1r9q7siBWN-lILhAUmZorltxzPZftpwwlwyU_DTNVZqNOK1L0nVCpkIE_y9B6jCac4p9GsTRdwf_m2tOtONePritgBETrXViHBSTqjwQIaB7Q8eDj9X_Z-LQ6jf2k1HGJl2yHDq6Kwj_pjmm6NETm5_e9I2Gr2y38BOyPV6AfKUaKZqaRt3I8e1Uy9XmK8eyAlahQDVAntkV-qqoPII8cI7X7TnFE5jx1cCDZECeqaSB6bymQoH3FZ_xuJHANdcZDOWFP6VhUw_v3lf3dZ23FTfVSzGvfr5gbw9fxfW9i1Oup1ObGovc3ApjeETFaNukAMdccsblXYlOtSrZdX6vK7HbsqTkNOvlJR1M8oION-VK2NVLHBXL3tr_Gmur7ptnDJOLvtWQbSO7KQ7VL1l4K4qw8QPnnZZKK8kU4s9hiZVjQoMTfFYt-zMKWKfcjxj3PD2G80RObRbQfdqaEVvlCfC61Fp80F4RCABlz03_cPsCUvG8k2Jt_pLZXx9RxbqqzfDTtlTyMA5QSWtXD_L2wX_r9kkL2M8K0DG0f1dSTZfwVhAXiVRtYZWwQWXniXGcHsmaxHKO6XiuZgoq8m6d99M-emeteC55r5FPbm1sa3P_zi8fUKDyglCkaN4-YCy_koELCMoJkS4lNbqiFhq5Ky5s-EMOXTiU9bq_5HE2P5sRHf3_ebrKDxporb65DzMIX4SIM0N7cXQzGPhV-aAsy00xDMg9YfRCCEVbVZDkQ07OJoj1ZzqklfeVnJ6jil9ZmOgGbqxnEtbRDqQbTYYPerxhpw8VY6nG4-x_8J8g9TQLUhs_Z-gBdG8XUz4XcAj_dmMaaBrVXi2cqK9Ufk3jklFMAcybWen9tGYyt458T0UqyYpFkUnvHg7Nlpa0Bg5KGSGDGLwTq8O404On88LpQt2aqfcc3PBr8OwjcDbgol_fll4nKI6oot3nAIxo8sHJDboba42DcWh8yv_Dp7LMqwuLR0FNjtsRrBOZdfsOMkBrjc9ZpiMbODClJuxRwrYuE3mvalRhKu69SR3NOoyMe1gewkovjVz6hQfODpncV9bnD9l45AbrgffINfadPrx2SabWdvlLcNJWX4zQan4rawlWXrqyWsjEvJmm_xytceGi0vwPqqo9iHIZUD969eKa2IY6TbkCogapoJrVibKv38Dqif2FJU31Q4XSBYl9XsXMdRLh5Xf8lliybPSBrvDNaNMzClssynj9SXGP8TAlVyofv3O2m2JEPC5lgIEJojWnOwyS4g-6DMzQ4359PJaewCMju6dxmHzPV1X3PfM-L-WxtQI_K_ztK0K-Zp1pFlbuLM-_-6_dtYxv42ixNtmsr-_-umfmmN6S7NdECoWNnLMbk-RFCkSqQTFZ5IdE6KU6UymJNf8iCeUB6FkNiiCd4AnxV3HgDRn-kb7ZxeUPyCHyD0Q-hGBK2V9p0krPa7AKWGR3bvA2xmTLEXbhRlSqeQ15EHST4ujjSqGfcx10_32WVQ4Z9_LBB8ogopDmec1tJzqiopyzDHOSfWyzDLG8UmGayyXIklxNh6p9EwLyE9Z7ucsFCsErlvLu6P6GtlNk5D39bKBh3zZCEMEfbERLX1TmJJQjxtUSXmQmTEohnDRM-7xmXHULTp4HtZFHyZn1sdJUOfKY2BGX0ceBCxpdFQRStX6tcJLEOvrFdPlKGYBrkXYUPzvrSMZ2XNPup1DI8i48cboEwz4uyu5vH-jJ4qTTQdqgdg1qdc93xSGF1Kd6II0c59efRe9yF9Bd33gY72H-qBRy7ClV3iWyFuOou0UOvFmA-csoaYuskx2m_PNNLwcArInPPvYHkquIC7gd9sb2mHhYvFkVRwJsAxUDj-xVhm8N4yfsRALfzE9q7O7460HzUN9lCbX1I_P8-pvXomZw_atEyIWs1BN-qRlQXu_Ywv9v08yeif2V9CpLpZnWTQvytTP7wG-mS8ri5_Dwe6Q8fUKP3dgnbVh8_pLdlQfby-AJvdh6X8wzvOW8qZc1YHLLBvuLAE5Rn8Mcwh76319BwxoCh_ph4z1J9XScMcvA-WWYAdyvzN6MiFr3zoiaJL0OO37VKVACX5DMjW8saxrsphReyayEN_JVxK-MfGslsh5t7HvL_8kOHTcNmdn7avX0ALOrh75-9wqPKS1vPN1O7TiLEGKXtomm_f-qSiZMrIInIP4eEPcvEV6z54Lr_UU1Ea5-_Aoq6So2o6z2zIUfLMarOUwNnIxXniJa6vLYS2X6WasjUUx1XhvF9UomoCNIuQIzNzko9WhZxW-KHwroaqBzltfgu_rlr2sFRr5epZBk3-58bay0aAzhmHx6wrHIBCK0ZyXszK3EAXs0saWnccXHcDMy5iNLka9L4dx0sTFtxlDdiS9Sy_LLpgydD7YJ7MesjW-6t-g_Y_K8Z6nXcnY66jpN4eO9MekJh4NX0ojMhFBpb3hfqPT1rFkWSZzseS97eA-9vuXDWKrQcPmBUdnNrcKqtNP7G5nSC511igEWz5VY5t4hJD-dAGyWc5bUO9OjsBfWDvTcPqfsobRgbytqXhusjBG7nW0Ql41xB5-4Rzu1KjFTsVXABeiWmP9hfVB6dohU8DxstcNQBoPpbS1RxPKkVC2dS4pljb4Q==', 'id': 'rs_68f6b258b3288190b1049a283fb9d7c6', 'format': 'openai-responses-v1', 'index': 0}])\n",
            "iteration : 1\n",
            "ChatCompletionMessage(content='', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageFunctionToolCall(id='call_VfEM7aJB1t88WT781vlu9DkF', function=Function(arguments='{\"query\":\"AI compute chips NVIDIA 2025 trends GPUs AI accelerators 2025 semiconductors AI energy efficiency\",\"max_results\":3}', name='tavily_search'), type='function', index=0), ChatCompletionMessageFunctionToolCall(id='call_UDFZUJTWLJ87LlxYNX9RPMVM', function=Function(arguments='{\"query\":\"watermarking provenance C2PA generative AI detection watermarking 2024 2025 standards\",\"max_results\":3}', name='tavily_search'), type='function', index=1), ChatCompletionMessageFunctionToolCall(id='call_ckUqX3wVkMsCyt4AdGcIqPGK', function=Function(arguments='{\"query\":\"copyright lawsuits generative AI training data 2023 2024 2025 legal cases authors lawsuit OpenAI Getty Images Stability 2023 2024 2025\",\"max_results\":3}', name='tavily_search'), type='function', index=2), ChatCompletionMessageFunctionToolCall(id='call_j2f8n1THxUBNive3bvAVfzyC', function=Function(arguments='{\"query\":\"workforce reskilling generative AI jobs automation reskilling programs 2025 PwC report reskilling\",\"max_results\":3}', name='tavily_search'), type='function', index=3), ChatCompletionMessageFunctionToolCall(id='call_ZCGUYR2Oxs3oB6QTGmXBnDSu', function=Function(arguments='{\"query\":\"AI governance standards ISO IEEE OECD AI principles 2025 guidance generative AI standards\",\"max_results\":3}', name='tavily_search'), type='function', index=4)], reasoning=\"**Producing JSON brief**\\n\\nI need to create a JSON creative brief based on developer instructions. It must include details like target audience, primary goal, professional tone, key takeaway, and research context. The developer's message also mentioned a list of letters about gathering context if user input is lacking, but I've already done that through web searches. Now, I have to ensure all topics are covered. If not, Iâ€™ll do more searches and summarize findings in the research context.**Creating a generative AI presentation**\\n\\nThe user has asked for a comprehensive presentation on the future of generative AI, which is a broad request covering various aspects like technology, applications across industries, economic impact, ethics, and more.\\n\\nThe search tool provided many relevant topics, such as models, applications, productivity, and trends from reputable sources. However, it seems I might need to include some hardware details, as well as topics like data governance, copyright issues, education, and international competition to ensure a well-rounded presentation.**Conducting targeted searches**\\n\\nI need to carry out some focused searches to fill in gaps for the presentation. Iâ€™ll look into topics like AI compute semiconductors, watermarking and provenance with C2PA, data licensing and copyright lawsuits related to generative AI, adversarial attacks, and workforce reskilling.\\n\\nIt seems wise to use the functions.tavily_search tool to tackle these gaps effectively. I might even consider running multiple searches at once with multi_tool_use.parallel to get results from different queries, aiming for around three results each.\", reasoning_details=[{'type': 'reasoning.summary', 'summary': \"**Producing JSON brief**\\n\\nI need to create a JSON creative brief based on developer instructions. It must include details like target audience, primary goal, professional tone, key takeaway, and research context. The developer's message also mentioned a list of letters about gathering context if user input is lacking, but I've already done that through web searches. Now, I have to ensure all topics are covered. If not, Iâ€™ll do more searches and summarize findings in the research context.**Creating a generative AI presentation**\\n\\nThe user has asked for a comprehensive presentation on the future of generative AI, which is a broad request covering various aspects like technology, applications across industries, economic impact, ethics, and more.\\n\\nThe search tool provided many relevant topics, such as models, applications, productivity, and trends from reputable sources. However, it seems I might need to include some hardware details, as well as topics like data governance, copyright issues, education, and international competition to ensure a well-rounded presentation.**Conducting targeted searches**\\n\\nI need to carry out some focused searches to fill in gaps for the presentation. Iâ€™ll look into topics like AI compute semiconductors, watermarking and provenance with C2PA, data licensing and copyright lawsuits related to generative AI, adversarial attacks, and workforce reskilling.\\n\\nIt seems wise to use the functions.tavily_search tool to tackle these gaps effectively. I might even consider running multiple searches at once with multi_tool_use.parallel to get results from different queries, aiming for around three results each.\", 'format': 'openai-responses-v1', 'index': 0}, {'type': 'reasoning.encrypted', 'data': 'gAAAAABo9rJrpwedkFOgiJ51NSkrfCXgQR1PsSLIPWSdJkUkAulUxQRMFNXAuVATRTJJzlY-o-qrIi1l3zvLcJUL1LwUKH7AxvBPfbS5W5aMguxZ8JvcXWNQjgk9lCj3qiFs9iQmFdiNcdt8jMJ9ryxgcdr23LRMbMnb_W0t3zi0Atcb2_8ZkQUvxIJ6nhPsiMivRnauolItBmdH8aOImnPJESRd4OieZvxoXAqJShLRb1RDQS7bUKg87ULgFtuKz7SOzFuRjk9MSb3s1q58Dyc59fU0h399Pm_vcg8PpxS09VuwopX3FJsXaol4-enaLojKlxfDKCWojcaFT6Luye1yuTz6etHaImSiDUJgQSIcx4NIK6AvyElfLwOtDmd_t2E4Ur3ffv6QHvnEfukhd8b0KfMxJsolBcrwR_tpbrA1GrKyM_PQrzbsNJvbGRqzykeGD_vRrOuYjM5_yHXzugxaAitl27QblVif7j5EJBfnSe_f_IkxdZxR5JToTk_TGSmVFU5989RWCASdBy63YYFd3eW_uPPEMZYUqVyCg3XbKxFTGvCspJfSdTQJ7wLTzzgk4CD8rI50ZnamwfPpjRCRCAgJyPrGy3fFwqwlCzPwr9OGQFuN0sflQ2JM3HqNSd4QsvjbCu3ctfizkdPeaULi1jhhOTKfTp0aBom7C-Gooqp_wlEXETCfydOlywk8E28fXil8clLG9cCXec5sDW_RsP2UcMHCyNfnAO3eRw0BJsDA1o-yng3K9WJ6E6jfc2QYpSnPgV_oyzAIihTN1a6Er8teUy4ko_M0X0LZXObvoaU1z9CPW2mtL1H6bL_mqqlalDiQ9DTyRINWm_nQUrGZlxGYKkMC4QApkL0zONua8OJaWgl0lXTWTqghxZ3zE6ptVcu78ydV2jjj1j4suwiDZARi4u-k7ZG2xnDQqXVR3TLGFpeofnZjCGNB050WJTCYdU3EVVEj1CXPop7ODZ6BZ0VLPNhR9cUBhxQgWPOe1cR1KfjZ5krtf4Q98A-BMAYyQCWP1HQ-xB_hwQFx-HXIVyV0qnfuLT4nK6jd0GDTgy77oUMMbRhPAec0_cH0SVE4NWo_rqCaBNDV2liKOiPF_TLTnoYgQ45S_9eLYLBblwCNkqOYXQOmUTDBwpOejhrnLZOk2rl-VyEH9i_r_cEOgHE9GmeZA25ITfzGHI2nUyXtvNEcco8mnG3_nvvE2LAxC6zUuOLPSbGmW5henxu-HsL9CzyhHbJox5CiSFSOGLGWuYTZcNI2-aKOE5cw2IFwjcgu9ZkAHS03d6-YDgG2KWL0xb0Is47PDor1LFnLE-2ljz4s3LIzXhqJ23U6w85vkLsXiF3ELTWK7I7PmK2IsaKOwuGDGll_7hODRcMDukTmCM2-92_J6fA-vqb45EXpCdxX8ii06ZVoLhNkJAqo89kR4i1KVZWAbPYte5mUkMHEDkUIfrLgM4m7r8V6Bfq2w92n7Jy2P_eqFhHubCL_Z3BnfsKEpJLZQPnPE1Wbqdm4R6rcn0KM_HmjqceIyGWPatiBFrvQoR6KuN7Z1_rVetDL004j6N3HDyR2IHFN4Oz4_C-2g_4H_cJ9R4FdSQnoiIowMiL2lOemTSOM_JtpXwfH_PSgTOdFkbTLTsJAH4bwnbZ9ulhkAUgEE8Ahmnoy7EMG4IFSZW-Q75Av3LQz7rhGshR1MuLDxLmxY7vECq6NxPgV5BMxu4-oxX3rZJOV5IfQOizOjz-34CYO9grw26iV61YVC8rdwIuytmoJ6qNdIVHa5iS3uNFTI_F4qvh5dQtEZcbBvgHIODeY4FvoOYC6gUNDrJz8iRsHrHpkD-KxCyWS_QXv9KT0aM03aa53rJjpcAOClozFAzFfNcM45kbYBYHUfBKauFCVU76WmWC883HdYFqGWZ0Kg_yra9gi7hmhtJn5n7btJVpy8vflMY4dkkW5PI_uzG36tJSBeLIKm6ZrGwuQPxyksJeOluqUJ2zg_eYkJ9QayAoW1hYJs7rbNGc5f8S7Vjoy3S0koKMMGZz7DAcUXWTVi21qlmmWG69zdMIXwUhAdywVeCzrLDvGKxMBMj9wnaK-58A8HmKo_GyKdmN0hZVmtfEA_blnrWze6ibeImy_FnBsciEYIPBiSKoE2tXRt4J_YG--pURB4pMFyoIyT2LqWKd0mGjzcceYQS6v45MYt2ZZu7QtCzQULILbdItfbSTmzWR9qURsxEeegoQ4Tze-LDUKZn3IOCKFfHTW4BOd5h4X56tndxNHLj_jizoxXAI6JcrWy9XOe54BFB6o1rWbyOzPLpOjl5shw9ZaMTfAOb83wDQX-dgoI8fa81Zyx3QVxMsyr8_zvoKPlzhDFtEP-eBnHl-Z7VKHiodKvkYWbBpjGJqlqKUKrA1r9q7siBWN-lILhAUmZorltxzPZftpwwlwyU_DTNVZqNOK1L0nVCpkIE_y9B6jCac4p9GsTRdwf_m2tOtONePritgBETrXViHBSTqjwQIaB7Q8eDj9X_Z-LQ6jf2k1HGJl2yHDq6Kwj_pjmm6NETm5_e9I2Gr2y38BOyPV6AfKUaKZqaRt3I8e1Uy9XmK8eyAlahQDVAntkV-qqoPII8cI7X7TnFE5jx1cCDZECeqaSB6bymQoH3FZ_xuJHANdcZDOWFP6VhUw_v3lf3dZ23FTfVSzGvfr5gbw9fxfW9i1Oup1ObGovc3ApjeETFaNukAMdccsblXYlOtSrZdX6vK7HbsqTkNOvlJR1M8oION-VK2NVLHBXL3tr_Gmur7ptnDJOLvtWQbSO7KQ7VL1l4K4qw8QPnnZZKK8kU4s9hiZVjQoMTfFYt-zMKWKfcjxj3PD2G80RObRbQfdqaEVvlCfC61Fp80F4RCABlz03_cPsCUvG8k2Jt_pLZXx9RxbqqzfDTtlTyMA5QSWtXD_L2wX_r9kkL2M8K0DG0f1dSTZfwVhAXiVRtYZWwQWXniXGcHsmaxHKO6XiuZgoq8m6d99M-emeteC55r5FPbm1sa3P_zi8fUKDyglCkaN4-YCy_koELCMoJkS4lNbqiFhq5Ky5s-EMOXTiU9bq_5HE2P5sRHf3_ebrKDxporb65DzMIX4SIM0N7cXQzGPhV-aAsy00xDMg9YfRCCEVbVZDkQ07OJoj1ZzqklfeVnJ6jil9ZmOgGbqxnEtbRDqQbTYYPerxhpw8VY6nG4-x_8J8g9TQLUhs_Z-gBdG8XUz4XcAj_dmMaaBrVXi2cqK9Ufk3jklFMAcybWen9tGYyt458T0UqyYpFkUnvHg7Nlpa0Bg5KGSGDGLwTq8O404On88LpQt2aqfcc3PBr8OwjcDbgol_fll4nKI6oot3nAIxo8sHJDboba42DcWh8yv_Dp7LMqwuLR0FNjtsRrBOZdfsOMkBrjc9ZpiMbODClJuxRwrYuE3mvalRhKu69SR3NOoyMe1gewkovjVz6hQfODpncV9bnD9l45AbrgffINfadPrx2SabWdvlLcNJWX4zQan4rawlWXrqyWsjEvJmm_xytceGi0vwPqqo9iHIZUD969eKa2IY6TbkCogapoJrVibKv38Dqif2FJU31Q4XSBYl9XsXMdRLh5Xf8lliybPSBrvDNaNMzClssynj9SXGP8TAlVyofv3O2m2JEPC5lgIEJojWnOwyS4g-6DMzQ4359PJaewCMju6dxmHzPV1X3PfM-L-WxtQI_K_ztK0K-Zp1pFlbuLM-_-6_dtYxv42ixNtmsr-_-umfmmN6S7NdECoWNnLMbk-RFCkSqQTFZ5IdE6KU6UymJNf8iCeUB6FkNiiCd4AnxV3HgDRn-kb7ZxeUPyCHyD0Q-hGBK2V9p0krPa7AKWGR3bvA2xmTLEXbhRlSqeQ15EHST4ujjSqGfcx10_32WVQ4Z9_LBB8ogopDmec1tJzqiopyzDHOSfWyzDLG8UmGayyXIklxNh6p9EwLyE9Z7ucsFCsErlvLu6P6GtlNk5D39bKBh3zZCEMEfbERLX1TmJJQjxtUSXmQmTEohnDRM-7xmXHULTp4HtZFHyZn1sdJUOfKY2BGX0ceBCxpdFQRStX6tcJLEOvrFdPlKGYBrkXYUPzvrSMZ2XNPup1DI8i48cboEwz4uyu5vH-jJ4qTTQdqgdg1qdc93xSGF1Kd6II0c59efRe9yF9Bd33gY72H-qBRy7ClV3iWyFuOou0UOvFmA-csoaYuskx2m_PNNLwcArInPPvYHkquIC7gd9sb2mHhYvFkVRwJsAxUDj-xVhm8N4yfsRALfzE9q7O7460HzUN9lCbX1I_P8-pvXomZw_atEyIWs1BN-qRlQXu_Ywv9v08yeif2V9CpLpZnWTQvytTP7wG-mS8ri5_Dwe6Q8fUKP3dgnbVh8_pLdlQfby-AJvdh6X8wzvOW8qZc1YHLLBvuLAE5Rn8Mcwh76319BwxoCh_ph4z1J9XScMcvA-WWYAdyvzN6MiFr3zoiaJL0OO37VKVACX5DMjW8saxrsphReyayEN_JVxK-MfGslsh5t7HvL_8kOHTcNmdn7avX0ALOrh75-9wqPKS1vPN1O7TiLEGKXtomm_f-qSiZMrIInIP4eEPcvEV6z54Lr_UU1Ea5-_Aoq6So2o6z2zIUfLMarOUwNnIxXniJa6vLYS2X6WasjUUx1XhvF9UomoCNIuQIzNzko9WhZxW-KHwroaqBzltfgu_rlr2sFRr5epZBk3-58bay0aAzhmHx6wrHIBCK0ZyXszK3EAXs0saWnccXHcDMy5iNLka9L4dx0sTFtxlDdiS9Sy_LLpgydD7YJ7MesjW-6t-g_Y_K8Z6nXcnY66jpN4eO9MekJh4NX0ojMhFBpb3hfqPT1rFkWSZzseS97eA-9vuXDWKrQcPmBUdnNrcKqtNP7G5nSC511igEWz5VY5t4hJD-dAGyWc5bUO9OjsBfWDvTcPqfsobRgbytqXhusjBG7nW0Ql41xB5-4Rzu1KjFTsVXABeiWmP9hfVB6dohU8DxstcNQBoPpbS1RxPKkVC2dS4pljb4Q==', 'id': 'rs_68f6b258b3288190b1049a283fb9d7c6', 'format': 'openai-responses-v1', 'index': 0}])\n",
            "ChatCompletionMessage(content='', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageFunctionToolCall(id='call_pKjviGhMRPHLsST8PFLqkkRf', function=Function(arguments='{\"query\":\"adversarial attacks model extraction generative AI 2025 security risks model theft\",\"max_results\":3}', name='tavily_search'), type='function', index=0), ChatCompletionMessageFunctionToolCall(id='call_wpzGFBwwnzkHldIPavFTE249', function=Function(arguments='{\"query\":\"privacy concerns generative AI training data regulation data protection 2025\",\"max_results\":3}', name='tavily_search'), type='function', index=1), ChatCompletionMessageFunctionToolCall(id='call_ELfmuAdKQ5CG3NRKuixI5R5S', function=Function(arguments='{\"query\":\"explainability interpretability generative AI models 2025 techniques\",\"max_results\":3}', name='tavily_search'), type='function', index=2), ChatCompletionMessageFunctionToolCall(id='call_baT6HYZ49kxeBJjALM0Em8t0', function=Function(arguments='{\"query\":\"MLOps generative AI deployment challenges production 2025 best practices\",\"max_results\":3}', name='tavily_search'), type='function', index=3), ChatCompletionMessageFunctionToolCall(id='call_1IgvccbRi65qCz9Ymz6QE9tz', function=Function(arguments='{\"query\":\"generative AI market size investment 2025 projections 2028 reports\",\"max_results\":3}', name='tavily_search'), type='function', index=4)], reasoning='**Gathering information for a brief**\\n\\nI need to create a Creative Brief JSON for the presentation team focused on the future of generative AI and ensure I\\'m covering all aspects of the topic. The developer instructed me to perform web searches if the user hasnâ€™t provided enough details. Iâ€™ve gathered information through multiple tools, covering areas like tech trends, applications, ethics, regulation, and more. Now, I have to confirm if every aspect is addressed or if I need to do additional searches to find more sources.**Identifying additional aspects for the brief**\\n\\nI still need to consider several important aspects for the Creative Brief on generative AI. This includes security issues like adversarial attacks, privacy, data protection, business models, and even healthcare-specific regulations. I should also cover topics like explainability, human-AI collaboration, metrics for evaluation, and risks related to existential threats. It looks like I might need more sources on privacy and other technical specifics. Finally, the JSON format has to include essential fields, along with a comprehensive list of findings and sources.**Addressing formatting and searching needs**\\n\\nThe developer mentioned some odd formatting for our task, which includes instructions to perform web searches if user input is incomplete. I\\'ve done some searching already, but I need to confirm if I\\'ve covered all necessary aspects. If there are gaps, I should find new sources. I think I should conduct a few more searches related to topics like adversarial security, privacy, explainability, and market impact. Using the multi_tool_use.parallel function could help streamline this process.**Planning additional searches**\\n\\nI\\'m planning to conduct searches focusing on key areas like AI model security, privacy concerns, explainability, deployment challenges, and market size projections for generative AI in 2025. Iâ€™ll use search queries such as \"AI model security adversarial attacks,\" \"privacy concerns in generative AI,\" and \"MLOps deployment challenges.\" Since the tools accept a maximum number of results, Iâ€™ll aim for three results per search using the multi_tool_use.parallel function to efficiently gather the information I need. Let\\'s prepare for five searches to cover everything thoroughly!', reasoning_details=[{'type': 'reasoning.summary', 'summary': '**Gathering information for a brief**\\n\\nI need to create a Creative Brief JSON for the presentation team focused on the future of generative AI and ensure I\\'m covering all aspects of the topic. The developer instructed me to perform web searches if the user hasnâ€™t provided enough details. Iâ€™ve gathered information through multiple tools, covering areas like tech trends, applications, ethics, regulation, and more. Now, I have to confirm if every aspect is addressed or if I need to do additional searches to find more sources.**Identifying additional aspects for the brief**\\n\\nI still need to consider several important aspects for the Creative Brief on generative AI. This includes security issues like adversarial attacks, privacy, data protection, business models, and even healthcare-specific regulations. I should also cover topics like explainability, human-AI collaboration, metrics for evaluation, and risks related to existential threats. It looks like I might need more sources on privacy and other technical specifics. Finally, the JSON format has to include essential fields, along with a comprehensive list of findings and sources.**Addressing formatting and searching needs**\\n\\nThe developer mentioned some odd formatting for our task, which includes instructions to perform web searches if user input is incomplete. I\\'ve done some searching already, but I need to confirm if I\\'ve covered all necessary aspects. If there are gaps, I should find new sources. I think I should conduct a few more searches related to topics like adversarial security, privacy, explainability, and market impact. Using the multi_tool_use.parallel function could help streamline this process.**Planning additional searches**\\n\\nI\\'m planning to conduct searches focusing on key areas like AI model security, privacy concerns, explainability, deployment challenges, and market size projections for generative AI in 2025. Iâ€™ll use search queries such as \"AI model security adversarial attacks,\" \"privacy concerns in generative AI,\" and \"MLOps deployment challenges.\" Since the tools accept a maximum number of results, Iâ€™ll aim for three results per search using the multi_tool_use.parallel function to efficiently gather the information I need. Let\\'s prepare for five searches to cover everything thoroughly!', 'format': 'openai-responses-v1', 'index': 0}, {'type': 'reasoning.encrypted', 'data': 'gAAAAABo9rKMTJBxiVhE7eSbu506Mi1oMUYl-iODYQBUvynSkJPmhSkZTiWL88me-KSXZvHQosh8GI0-0znSpI7qSy8jom9d9SNJpTXpkMXc-pQgPQCnxDDpwH1ezk123ByxGq_uA9ffgjvNVN-90qgXcOKvsybdM1tNAAEJlBguN-o2thovyGfWOPCHSC6kQPpzwGpiwW5x6SNnotL9XqGxxia97nGmzDbT6XzkbUuc0VRXIlfWZwk861xJBMcZDAGuV-ZG0j-1eVzK4QH2n5NErpMBPCCY3N3Gs2EtS_bFvB0wwoL2xQ08SFWldkzrgalhx5Yg8R29ul28QtLxhWa1S1nW-xd4TBDk-qwzXQ70TA-H8Urz-L_JV7KmQOIIv_uerW-fXLSqiBnj0m6biqHi6XnyT0DUdYIulN4Ye_qdg6GBQg5jVc1AaG5bQ6am1mzVUrcOClaP_7eroxYPHX9DQWQi1JyOQdVzeHl8QMxC9bFMpVquoewQ3iJ7q6Bd_TmFGG9cWj5ukxy_7tXlQ3uxz9Dldou0Uo-5tu9NU5Be0DMDMCR2CuBmYeQeBXCGUUIQTv8Ll6qjgLFGg-6cb5l6ol85X61mE6vj2483ztQTWDDbRcwPl8fil-d5d_MVFshhy5QnRJwZ4lF_C8yAa2qEqSuhHAO9szP6KQNTaEl4PmDTKQVDomzffidHTPvNa8ffZO51hOIjQvnL8t203fTHqYeWmqhvFebgCLw8BijABMhYJQeLD2KVTuiIIfR7Z6Jtw4u02l122Ud1unXKWVMLHMjTgVnNS6Unfuj2lqk8uPou1yMatK8EaEwQSNnfvixeeg_cY_RvxedpPvGw3Tc_h3Mu8nUXy8Z68c8NO3tCJay1Akc9qAgz4Yk4LmdwsgDjqoNhVGw7rZjJQrfro2oTrsYXbcEyfcYvd0aeOGEOvOyLJmUS0gq2DyMgStRG0jvMOCzC08li1J6OSpc6JKi5ZdLTFP-NIMwMIJLQmrQ_csRr5V7KNgin2x8eIOM8M9zp89qsb3d7dMEKirwi5Dik3LCRWfRNitBa6w9zTp-Wu4FY9B8Ahj0ws6poaELXaNe5gCLSkyD6-dCrwNctmPzvYrF5-nQ_o03q_4sNcHs4VSwzagWerF07J40ICJO3E1EgGuaSidu8fVt7fQb9mPZQiRol-tOSet5ieCgbm7CcG4nCRtsV9B10KFCn60cCvGw4KcKLLCIdIx5XvLW_jjl-6TrZEk42p7eNE3slay_HwsRE_MuXV2V-6xZ02G8OUjuE_-aK2LH9Az90qPkqYEnIWw4l9UBHCeFFOlUQAi1Tq5UKLVazoP4MIfltRizt24R-ttUv5GltpmgUNDNMV3kqD-p7zcsc-vs-QdwCCQvI58rlX2_7OIYdAFRNCKtNRo40Z6RnamFyB0H6f0NUQodVcrPyGqw6A_UoBJg_2uq0nIvGIrB-7qHbJlhwfj7L2pywGQbCFHx6SENxfKYYcu79P3fkH_Q2Jy6QVhuslXx-m2HLlNFdU-E0KzTo0fssofn6eVhuZ67QXiM4MWaaMx_CRzfDNBElET1suyJ6g5PfvnPTTsXM46E5rBOUNgcpA__i5o-vpM7GaKaueYDCt_R7F0IgHdBwjd-mnAi42nQDG4BcUTT81KoV-czG_kFZmHNgP7UrI3Z6EkeKCVM0KXePvpa2_m1RJR9EgrpdfufzoqZDPzEjD9lHlK8yNWTi1fc08rj97gkFB0I30sn3XVvZPLpgytZuJLcZEFLLJoy3zE7AyhJMLoB_aKHLW-f8JOWGeNvQarY-lP4bnEoJprBYAoDjUx_dhTKTpW73buNTK3IrXWfDW1kdzvMq13jdk-b8-SbPrx44TxqXL6pCy3JBG6HLRKTk-mnelmMyTs0qkbWXUlnZ2CEyycmRquaQZjeAOZT3T15bjTMmxdX1JOh2xuTuZYqH_PKscJV3JrxYeAyKXtGmprPjWtkwpGVmDCPVdlDUe9lOQpL2o4uoEvyftlEpQuvYFXheIS7AGQwqPFrK4xlhb_FJoZVlksN_L9k1-2eNoRYdDdG1Ks7fxzBSZ4pIGro775JsdrQVCKmZEaMZ79vN1z0vlWoyFIySE3exmnG4wn7FG4M-5r-T9j5O3kHdzAPMpw8TN06SqEriXMxkPADfa_jGClzoNmlLF3w7r_nXRwb3XeKMELG-ODCFGkYQYaa1w2UPfM5KO3K0EufjF2xf91KZPYZkmn_uzilYd151VDvuhTKEXk7RdBEOb62lNVQG0gPc6BWrxXx81oTTEIuYjNbq9dOyl0p5yBVEgnAPRmgK9cFWbdf1SKhmbs7q0v0ciaF_yORdmAlJWAZthQra7lOJHRO4hk-uCe16cGsEYXd8SfGKGEZG7slOve8WMu035_YfOJxQlXgcCFrgYzhzELDoQ_Q5HUt9RdlO2iFWfOSu8KP4S0I0d6dC00X33WANAB35AUJZaw3VkER8Hxg_gVZkW7Y3NJlFAN4KfBrxV9exQpUov7kkoJXq3PHbBkGoZpEEe0yjIKVx21-QP10MrdLAvI_YAoYDH1xBugCTXuaV3AcpZK7DYt2S3fxBmQljvSwKyz7sydWJhF1XQ8ynFIaEXFoqw0Nzv2yzzxqnpNIN3OmMn8t1QOKcog8MFXloe3yPUL48RWDCqQaO-zzHw8SMjsi8urcHDUxfjaMGsI5dptr0LJPOIkHRLu9tjLoXOqwcj9rXDhQe3vEKMTgeca-Qd_VXCATnZXg4M7q1YK0OF1_w6gopotYHSYrCgXisFeCnnodjW5zGzYihf9p52ep7cbEIdFXZaCWMkQVqzgO2-grG-t0xY3kcB1WS7Rfzl6nzz1w1RarZzWqHcjgdq-zIAG334K6rvRFPfprfsLFeskaCOCTX1KxGO6EcJNxCj9qLthHyZ7rUrivvRn8zQOKMbPa7izB_kNYlQgcIWJ50tTEuWpicXTYVwoV1c3z5ung5IHxvWVSJaS6Kcr3bYdGcK5YVUR9aoUDKyEAAW2v8lEend3zRdhbIFIrJC4AH6F3iFx2WIl0YMIKaUKA6WjJl2x2MTnZkFH5I_88hSud2XiTtzNsibu2aeUQOOkdtngbF_AYzwsTmo6Tz59Kw8L4tl-XLI4s4iHiTTEHd5Q7VxRgRwpOnWYNCzQPU18JfqasaTXUza7pgrupP5H_kG_-j8a_UpvwBtVF9iFk162nvwHkykhu0JVVyyzYd2OfrTM1lu5kGIAL6EmVIgVQgUosGCSupvB3ru3Fqq7JHsAaDX6Ngy0QVHq4HrFMCJrb-Cr7fZW38M1SB1KWEQnypaAHQ0m9uWgl9Q6xb7fjSV1WtDF8-3oapQtdJ8gsST1TkJ0FEi5X_milic81fpe9CN17CqoF8pcLtrmAcaabVWM8Gnx9IWvB1QJZZX1P5_CQClvnN83utdq7zXw1BQmMrsEOm42_epZJfHKSvbSMNZ_1XuOyl0v4tkbbgbU8rRuI9GynKea4zP7PwLR6PXXcLTA8jaItDyynZoVRr0EAvtQzdTpTuBnfB5PG_f1cZrHFT7JQz_ec4EJHHI4yGxgS3O9K1mr-WcZjgxyFGy2hR1uuTpSQ1wVjkxFfEhxnp4r9wL_hSN1bjYphfCUg7wnOnOYugY_gxc5Dh49bHMZcVkuDflT9gEJ7jqCESHVnaCpivCGiN5tcD6p03rnbedtlvcnMVN4EPS7yMZFpCl79CMsOzowlIO2Kprycpus95RgIbHlMaCvhNtLAkNNUjEvRFkbCEG8FSgj5wnILmLfXx08eRlEvFts-Y7v67IIqyRxmS_gA5Sya9xTe851ZJL_MfIV3MwDRaFmNPGVbrldZuo28QjbzTDRuGL8_dAhuZ3CWvE72GVojId6Z3C5DaKv9vUhMtpM0evDgh9l_5BSNXmrBtDq19ti8KyES3xYPMwdvljEstEzhtNrgo-6RvN0YpKzipSA6mHYSCvDvt8Ok_nvhq2etS3P5L1_5vcefrUo2MiwaFWBkdE-VX3PTcwKdHbwvjQqMKwe_b372G5l6K75Gope6domW7JgFALFG92VGanzWl50hZwoF8tIq0aMmbdClG6Yz4cigODByXr8oyy-F5n-wA0WmYaH3nnC96JkDsCBGSecJQZPQ3LBSCsLSh_FSsz0nWrW8fKtidcFPyj8tBsug9Vkbj8ef_tGysxk7w4QakDeyufivfhf2sqNNTp4A3vuB2_JeQpFRE9HKU5neTTdIo2qF8GKSVOZJ2o9hxRVubjehDsZT4KC4O5Qsq0hMGhml43EZ1bYw_Yc2HyOzdSkKf_NZ6l0_LoNqr39QeVvq5GlUx5LhysEvwKHtdGva0C8e16eTJ4p_KeChYlXDangtHupvfrDoYMXxtNtj_y7jQ1GEC7ztojOooYDM_CTq0WKzrfhOZLUwFfGY4BQaHvghhlfPQTZN5P_gVBCbpKwZa4VspObhK4HyJdqNQzmrOSF-lmMRFIlHfJ6Wgf5EObKEiKOGhMFqA37hZjli6RsGP9xE4Fx0qr2O1z__F1WKZxS-oO47rylqU54Tv6YQoBTt-0OiOc5oB-qeLZ9giXXjostY44cOJSoX6IBELzAM_NeQFg8J-xKnXRaCEA-wkpcdzW2xGv91liWzw5DRYCfEoqlWFQedaQyL64-KrpKOzly4GqAOXAxNqUrkE7_ZoetvGBmuzQcQYqOZjjPVQYC91PX9OYWoF1c3HwV9ScdKUo3dPavQH9kXSA8JMPECBUQ7VfBQipQDPn6oI97lZBEAm1Zg_u6-8vYxmFINiS22RiFAhTDOwDC_M_mwB7fasPCC2ove5C5kWwnsHRsWGoZ7-JLmWKcGCgwF7Dd__D36D0v7JW_L_gzXOsuoPSAPbKsTJZ7lqEJUz5_57PmsbyXO16q_sE49Gb8SSid2XcYJIh_GAltHo9LDW7y05Tvp8fp3rDzLHPStw063GcUz4xdPUuVuV7RlF6nxAh2i-16WTQTQBrJYMUzVZlzDNF7wkQd9DwON_8h8AcYnFl8MVLFRb91lFYUdVowo5mppnK1rHUvMfFLqtvTUGpqLW0M69Ba0AEicPVKdnQcunOeYZ36VRXFY1fz45MveMdT8tNCLA528wxi5MDbanUM-DJ2g9WQs_nCJjiv5cLHF-NwB3janjoJt8RV_dnFvXhh_TjrjQt_eXavK1Gak7VVYbY4DF_oicPvqy6eExriXa7KH2WQ==', 'id': 'rs_68f6b278dd348190bb779b3d16a423cc', 'format': 'openai-responses-v1', 'index': 0}])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "startegist_context_manager.messages[-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klJLyGL7pFgq",
        "outputId": "a8f890b2-3c0a-4e1a-e0f9-9d64899180cb"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'role': 'assistant',\n",
              " 'content': CreativeBrief(target_audience=['C-suite executives (CEOs, CTOs, CIOs) evaluating strategic investment in AI', 'Product and engineering leaders planning GenAI adoption', 'Policy makers and compliance/legal teams responsible for AI governance', 'Investors and business development leads assessing market opportunities', 'HR and learning & development leads planning workforce reskilling', 'Practitioners and technical leads who will implement solutions'], primary_goal='To inform stakeholders about the full landscape and trajectory of generative AI (technology, applications, economics, regulation, risks, and infrastructure) and to persuade decision-makers to adopt a balanced strategy that invests in capability, governance, sustainability, and workforce reskilling.', professional_tone='Business-formal: authoritative and strategic but accessible to both technical and non-technical stakeholders.', key_takeaway='Generative AI is rapidly evolving into multimodal, agentic, and widely accessible systems that will materially boost productivity and create market opportunities â€” but organizations must pair investments in models and infrastructure with robust governance, IP strategy, sustainability plans, and reskilling programs to capture benefits while managing legal, ethical and energy risks.', research_context={'summary': [{'topic': 'Core technologies & architectures', 'insights': 'The field is converging on multimodal foundation models that combine transformers and diffusion approaches (and emerging hybrids like Multimodal Diffusion Transformers). Advances in model architectures and training methods continue to improve generation quality across text, image, audio and video.', 'representative_sources': ['Once Upon a Prompt: The Multimodal Magic of LLMs and Diffusion (Medium)', 'Introduction to Diffusion Models, Autoencoders and Transformers (preprints.org)', 'Emergent Mind: Multimodal Diffusion Transformers']}, {'topic': 'Agentic AI & autonomous agents', 'insights': 'Agentic AI (autonomous AI agents/AI colleagues) is a major growth area: tools that plan, execute multi-step tasks, and orchestrate other services are being productized by major cloud and software vendors and startups.', 'representative_sources': ['CRN: The 10 Hottest Agentic AI Tools And Agents Of 2025', 'McKinsey Technology Trends Outlook 2025']}, {'topic': 'Applications across industries', 'insights': 'Generative AI use cases span healthcare (clinical notes, drug discovery), finance (Robo-advisors, fraud monitoring), media & entertainment (content creation, personalized experiences), retail (visual search, personalization), manufacturing (design, predictive maintenance), and education. Many sectors report high productivity gains where AI is adopted.', 'representative_sources': ['Coursera: 20 Examples of Generative AI Applications', 'Hatchworks: Generative AI Use Cases Across Industries', 'Synthesia: 50 Useful Generative AI Examples in 2025']}, {'topic': 'Economic & workforce impact', 'insights': 'Studies estimate significant GDP and productivity upside from GenAI but also large exposure of tasks/jobs to automation. PwC and university models (Wharton) show productivity uplifts and highlight the need for reskilling; evidence shows wages rising for AI-complementary roles.', 'representative_sources': ['PwC 2025 Global AI Jobs Barometer', 'Wharton Budget Model: Projected Impact of Generative AI on Future Productivity Growth']}, {'topic': 'Open source vs proprietary dynamics', 'insights': 'Open-source foundation models (e.g., LLaMA variants, Mistral) are closing performance gaps with proprietary offerings; this affects cost, control, and vendor strategy â€” organizations must choose a path aligned with IP, compliance and hosting needs.', 'representative_sources': ['Open vs. Closed AI: What Business Leaders Need to Know in 2025', 'Open-Source vs Proprietary trend analyses (industry reports)']}, {'topic': 'Regulation, legal risk & IP', 'insights': 'Regulatory momentum accelerated with the EU AI Act (risk-based requirements), national AI legislation activity, and legal precedents around training-data copyright (e.g., Getty vs Stability AI). Compliance, transparency, and provenance obligations will shape deployments.', 'representative_sources': ['EU AI Act summaries and compliance guides', 'NC SL and national roundups of AI legislation (2024-2025)', 'News coverage and analysis of Getty Images vs Stability AI and other copyright cases']}, {'topic': 'Ethics, deepfakes and provenance', 'insights': 'Deepfakes and synthetic media create reputational, political and legal risks. Industry responses include provenance standards (C2PA), watermarking, detection tools and policy proposals; ethical frameworks from major vendors (e.g., Microsoft) emphasize transparency and bias mitigation.', 'representative_sources': ['Digimarc/C2PA implementations', 'Academic literature on deepfakes and legal/ethical frameworks', 'Microsoft Responsible AI materials']}, {'topic': 'Standards and governance', 'insights': 'International standards bodies and governance frameworks (OECD, ISO/IEC, NIST guidelines) are forming practical guidance for risk assessment, documentation, explainability, and accountability. Organizations should align with these to lower compliance friction and reputational risk.', 'representative_sources': ['OECD: Governing with Artificial Intelligence (2025)', 'ISO/IEC and NIST advisory materials']}, {'topic': 'Infrastructure, chips & energy', 'insights': 'Hardware advances (NVIDIA GB200/Grace, next-gen GPUs, specialized AI chips) improve performance and energy efficiency, but AI compute still has heavy electricity and water footprints. Optimization (model efficiency, sparsity, quantization), edge/cloud balance, and renewable-powered data centers are key to sustainable scale.', 'representative_sources': ['NVIDIA engineering blogs', 'arXiv analyses of AI supercomputer energy efficiency', 'NPR and IEA reporting on AI energy/environmental impacts']}, {'topic': 'Detection, watermarking & provenance tech', 'insights': 'C2PA manifests, invisible watermarks, and signed provenance metadata are being integrated into content pipelines; adoption will accelerate as regulators and platforms require traceability for synthetic content.', 'representative_sources': ['Digimarc press release on C2PA 2.1', 'Research on watermark adoption and limitations (arXiv)']}, {'topic': 'Market forecasts & investment', 'insights': 'Large analyst reports (IDC, McKinsey, StartUs, Exploding Topics) project large enterprise AI spend in the hundreds of billions and identify GenAI as a top investment priority for 2025â€“2028. Venture activity strongly favors companies building agentic, multimodal, and developer tools.', 'representative_sources': ['IDC Futurescape GenAI predictions', 'McKinsey Technology Trends Outlook 2025', 'StartUs Generative AI Report 2025']}], 'notes_on_sources_and_dates': 'Research gathered from diverse industry reports, academic preprints, major press outlets and vendor publications between 2023-2025 (representative sources include PwC 2025 AI Jobs Barometer; Wharton Budget Model; EU AI Act summaries; Digimarc/C2PA; academic preprints on diffusion and multimodal models; NVIDIA engineering blogs; CRN/industry coverage of agentic AI; Synthesia/Coursera/Hatchworks use-case surveys; and multiple analyst forecasts). Use these sources for slide citations and the recommended reading slide.'})}"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the base system prompt\n",
        "STRATEGIST_SYSTEM_PROMPT = create_expanded_context(\n",
        "    base_prompt=\"\"\"\n",
        "    Your job is to take a user's topic and create a 'Creative Brief' for the presentation team.\n",
        "\n",
        "    You must define:\n",
        "      - target_audience\n",
        "      - primary_goal (e.g., 'to inform', 'to persuade', 'to get funding')\n",
        "      - professional_tone (e.g., 'casual', 'formal', 'academic')\n",
        "      - key_takeaway\n",
        "      - research context\n",
        "    \"\"\",\n",
        "    role=\"You are an expert communications strategist and audience analyst.\",\n",
        "    output_format=\"JSON\",\n",
        "    constraints=\"If the user's input lacks important details, perform the necessary web searches to gather basic context before generating the brief.\"\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "strategist_messages = [\n",
        "    {\"role\": \"system\", \"content\": STRATEGIST_SYSTEM_PROMPT},\n",
        "    {\"role\": \"user\", \"content\": \"Create a presentation about the futur of generative ai, all aspects\"}\n",
        "]\n",
        "tools_executor={\n",
        "    \"web_search_tool\": web_search\n",
        "}\n",
        "startegist_context_manager = ContextManager(strategist_messages)\n",
        "strategist_agent=strategist_agent(\"Create a presentation about the futur of generative ai, all aspects\",startegist_context_manager,tools,tools_executor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "Kor_bnjFMmxb",
        "outputId": "6310243c-a803-45d1-bfd0-afdf9c03c265"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content='', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageFunctionToolCall(id='call_aXT8vtUH53fbSfoiuwzOnzuZ', function=Function(arguments='{\"query\":\"future of generative AI\",\"max_results\":3}', name='tavily_search'), type='function', index=0)], reasoning=None), native_finish_reason='completed')\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Choice' object has no attribute 'tool_calls'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pydantic/main.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    982\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 983\u001b[0;31m                         \u001b[0;32mreturn\u001b[0m \u001b[0mpydantic_extra\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    984\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'tool_calls'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1929019733.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m ]\n\u001b[1;32m     24\u001b[0m \u001b[0mstartegist_context_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mContextManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrategist_messages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mstrategist_agent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrategist_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Create a presentation about the futur of generative ai, all aspects\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstartegist_context_manager\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3202658281.py\u001b[0m in \u001b[0;36mstrategist_agent\u001b[0;34m(topic, startegist_context_manager, tools)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;31m# Step 2: Handle LLM function call if it occurred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtool_calls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mtool_call\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtool_calls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m           \u001b[0mfunction_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtool_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pydantic/main.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    983\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0mpydantic_extra\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{type(self).__name__!r} object has no attribute {item!r}'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Choice' object has no attribute 'tool_calls'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain.agents import initialize_agent\n",
        "tools = [web_search]\n",
        "\n",
        "agent = initialize_agent(\n",
        "    tools,\n",
        "    llm_client,\n",
        "    agent_type=\"chat-zero-shot-react-description\",\n",
        "    verbose=True,\n",
        "    handle_parsing_errors=True\n",
        ")\n",
        "\n",
        "# 5ï¸âƒ£ Run the Strategist Agent\n",
        "topic = \"Create a presentation about the future of generative AI, all aspects.\"\n",
        "response = agent.run(f\"{STRATEGIST_SYSTEM_PROMPT}\\n\\nTopic: {topic}\")\n",
        "\n",
        "print(\"\\nðŸŽ¯ Structured Creative Brief:\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "l_IU-1xJ96Ur",
        "outputId": "2747dd42-47dd-472d-d775-41b5400b632e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2986039617.py:4: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
            "  agent = initialize_agent(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ZeroShotAgent does not support multi-input tool tavily_search.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2986039617.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtools\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mweb_search\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m agent = initialize_agent(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtools\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mllm_client\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/_api/deprecation.py\u001b[0m in \u001b[0;36mwarning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m                 \u001b[0mwarned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m                 \u001b[0memit_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mawarning_emitting_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain/agents/initialize.py\u001b[0m in \u001b[0;36minitialize_agent\u001b[0;34m(tools, llm, agent, callback_manager, agent_path, agent_kwargs, tags, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0magent_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAGENT_TO_CLASS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0magent_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent_kwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         agent_obj = agent_cls.from_llm_and_tools(\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mtools\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain/agents/mrkl/base.py\u001b[0m in \u001b[0;36mfrom_llm_and_tools\u001b[0;34m(cls, llm, tools, callback_manager, output_parser, prefix, suffix, format_instructions, input_variables, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAdditional\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0mto\u001b[0m \u001b[0;32mpass\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \"\"\"\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_tools\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         prompt = cls.create_prompt(\n\u001b[1;32m    141\u001b[0m             \u001b[0mtools\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain/agents/mrkl/base.py\u001b[0m in \u001b[0;36m_validate_tools\u001b[0;34m(cls, tools)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_tools\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBaseTool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0mvalidate_tools_single_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             msg = (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain/agents/utils.py\u001b[0m in \u001b[0;36mvalidate_tools_single_input\u001b[0;34m(class_name, tools)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_single_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{class_name} does not support multi-input tool {tool.name}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: ZeroShotAgent does not support multi-input tool tavily_search."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "startegist_context_manager.messages"
      ],
      "metadata": {
        "id": "k0W7KCyUfJn_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84d72213-b9a0-4858-f138-21a90742223a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'role': 'system',\n",
              "  'content': \"You are You are an expert communications strategist and audience analyst..\\n\\n\\n    Your job is to take a user's topic and create a 'Creative Brief' for the rest of the team.\\n    You must define:\\n      - target_audience\\n      - primary_goal (e.g., 'to inform', 'to persuade', 'to get funding')\\n      - professional_tone (e.g., 'casual', 'formal', 'academic')\\n      - key_takeaway\\n\\n    Return your response as a structured JSON object.\\n    \"},\n",
              " {'role': 'user',\n",
              "  'content': 'Create a presentation about the futur of generative ai, all aspects'},\n",
              " {'role': 'assistant',\n",
              "  'content': '{\"target_audience\":\"Tech professionals, AI researchers, business leaders, and innovation enthusiasts interested in the future developments and implications of generative AI.\",\"primary_goal\":\"to inform and inspire the audience about the comprehensive future prospects, challenges, and opportunities of generative AI across various sectors.\",\"professional_tone\":\"formal with an engaging and forward-looking approach\",\"key_takeaway\":\"Generative AI is poised to revolutionize multiple industries by enhancing creativity, productivity, and problem-solving, but it also requires careful consideration of ethical, societal, and technical challenges to ensure responsible and beneficial adoption.\"}'}]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RESEARCHER_SYSTEM_PROMPT = create_expanded_context(\n",
        "    base_prompt=\"\"\"\n",
        "   Using the 'Creative Brief' from the Strategist, your job is to find the most relevant facts, statistics, case studies, and supporting data.\n",
        "   You must focus only on information that supports the goal and key takeaway for the target audience.\n",
        "    \"\"\",\n",
        "    role=\"You are a diligent research analyst. \",\n",
        ")\n",
        "researcher_messages = [\n",
        "    {\"role\": \"system\", \"content\": RESEARCHER_SYSTEM_PROMPT},\n",
        "    {\"role\": \"user\", \"content\": \"Create a presentation about the futur of generative ai, all aspects\"}\n",
        "]\n",
        "startegist_context_manager = ContextManager(strategist_messages)\n",
        "\n",
        "brief = llm_client.structured_completion(messages, CreativeBrief)\n",
        "startegist_context_manager.add_message(\"assistant\", brief.model_dump_json())\n",
        "print(\"ðŸŽ¯ Structured Creative Brief:\")\n",
        "print(brief)\n",
        "from langchain import hub\n",
        "from langchain.agents import AgentExecutor, create_react_agent\n",
        "\n",
        "prompt = hub.pull(\"hwchase17/react\")\n",
        "tools =  [\n",
        "    web_search\n",
        "]\n",
        "\n",
        "agent = create_react_agent(model, tools, RESEARCHER_SYSTEM_PROMPT)\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools)\n",
        "\n",
        "agent_executor.invoke({\"input\": \"hi\"})\n",
        "\n",
        "# Use with chat history\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "\n",
        "agent_executor.invoke(\n",
        "    {\n",
        "        \"input\": \"what's my name?\",\n",
        "        # Notice that chat_history is a string\n",
        "        # since this prompt is aimed at LLMs, not chat models\n",
        "        \"chat_history\": \"Human: My name is Bob\\nAI: Hello Bob!\",\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bS_uptiSfRm_",
        "outputId": "4a10d458-946a-4441-d9cd-b605443365cf"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'target_audience': 'Tech professionals, AI researchers, business leaders, and innovation enthusiasts interested in the advancements and implications of generative AI.',\n",
              " 'primary_goal': 'to inform and inspire the audience about the future developments, applications, challenges, and ethical considerations of generative AI across various sectors.',\n",
              " 'professional_tone': 'formal yet engaging, balancing technical depth with accessibility to appeal to a diverse professional audience.',\n",
              " 'key_takeaway': 'Generative AI is poised to revolutionize multiple industries by enabling unprecedented creativity and efficiency, but its future success depends on addressing ethical challenges, ensuring responsible use, and fostering interdisciplinary collaboration.'}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "research_agent="
      ],
      "metadata": {
        "id": "_xcjid8F2qq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9a04980f"
      },
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "# Define the state\n",
        "\n",
        "class GraphState:\n",
        "    def __init__(self):\n",
        "        self.ContextManager = ContextManager(messages=[])\n",
        "        self.ContextManager.add_message(\"system\", \"You are a helpful assistant.\")\n",
        "        self.LLMClient= OpenAIClient(\n",
        "          api_key=\"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\",\n",
        "          base_url=\"https://openrouter.ai/api/v1\",\n",
        "          model=\"gpt-5-mini\"\n",
        "        )\n",
        "        self.next = None\n",
        "\n",
        "# Create an instance of StateGraph\n",
        "graph = StateGraph(GraphState)\n",
        "\n",
        "# Define a node to handle tool calls\n",
        "def call_tool(state: GraphState):\n",
        "    # In a real scenario, this node would execute the tool calls\n",
        "    # and update the state with the results.\n",
        "    print(\"Calling tool...\")\n",
        "    state.messages.append(\"Tool called\")\n",
        "    state.next = \"end\"  # Simple flow: tool calls lead to the end\n",
        "    return state\n",
        "\n",
        "# Add the defined nodes to the StateGraph instance\n",
        "graph.add_node(\"orchestrator\", orchastrato)\n",
        "\n",
        "# Set the entry point of the graph\n",
        "graph.set_entry_point(\"tool_caller\")\n",
        "\n",
        "# Set the finish point of the graph\n",
        "graph.set_finish_point(\"tool_caller\") # For this simple example, the tool caller leads to the end\n",
        "\n",
        "# Compile the graph\n",
        "app = graph.compile()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c315a38"
      },
      "source": [
        "## Define tools\n",
        "\n",
        "### Subtask:\n",
        "Define the tools that the orchestrator will use.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pQOWWSl099E0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11246878"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the necessary decorator and define the dummy tools as instructed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdc73464"
      },
      "source": [
        "from langchain_core.tools import tool\n",
        "\n",
        "@tool\n",
        "def tool1(query: str) -> str:\n",
        "    \"\"\"Calls tool1 with the given query.\"\"\"\n",
        "    return f\"Tool1 was called with: {query}\"\n",
        "\n",
        "@tool\n",
        "def tool2(query: str) -> str:\n",
        "    \"\"\"Calls tool2 with the given query.\"\"\"\n",
        "    return f\"Tool2 was called with: {query}\"\n",
        "\n",
        "tools = [tool1, tool2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cf97fa6"
      },
      "source": [
        "## Define the graph\n",
        "\n",
        "### Subtask:\n",
        "Define the LangGraph graph with nodes and edges representing the orchestrator pattern.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "961dd766"
      },
      "source": [
        "**Reasoning**:\n",
        "Import necessary classes, define the state, create the graph instance, define and add nodes, set the entry and finish points, and compile the graph.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "832b1f43"
      },
      "source": [
        "## Run the graph\n",
        "\n",
        "### Subtask:\n",
        "Run the defined graph to demonstrate the orchestrator pattern.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00347b58"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the initial state and invoke the compiled graph with this state. Then print the result.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "aad339d7",
        "outputId": "a21a1c0f-4b0f-41f4-b7eb-d7fe9cc919a2"
      },
      "source": [
        "# Define the initial state\n",
        "initial_state = GraphState()\n",
        "\n",
        "# Invoke the compiled graph\n",
        "result = app.invoke(initial_state)\n",
        "\n",
        "# Print the output\n",
        "print(\"Final State:\", result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidUpdateError",
          "evalue": "Expected dict, got <__main__.GraphState object at 0x7cd245aca5a0>\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_GRAPH_NODE_RETURN_VALUE",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidUpdateError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2711712098.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Invoke the compiled graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Print the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/main.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[0m\n\u001b[1;32m   3092\u001b[0m         \u001b[0minterrupts\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInterrupt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3093\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3094\u001b[0;31m         for chunk in self.stream(\n\u001b[0m\u001b[1;32m   3095\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3096\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/main.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2677\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch_cached_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2678\u001b[0m                         \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2679\u001b[0;31m                     for _ in runner.tick(\n\u001b[0m\u001b[1;32m   2680\u001b[0m                         \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2681\u001b[0m                         \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_runner.py\u001b[0m in \u001b[0;36mtick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m                 run_with_retry(\n\u001b[0m\u001b[1;32m    168\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                     \u001b[0mretry_policy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_retry.py\u001b[0m in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;31m# run the task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mParentCommand\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONF\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONFIG_KEY_CHECKPOINT_NS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/_internal/_runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m                 \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRunnable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_write.py\u001b[0m in \u001b[0;36m_write\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mwrite\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         ]\n\u001b[0;32m---> 84\u001b[0;31m         self.do_write(\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mwrites\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_write.py\u001b[0m in \u001b[0;36mdo_write\u001b[0;34m(config, writes, allow_passthrough)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;31m# can move this to a finally block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mwrite\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTYPE_SEND\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONF\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONFIG_KEY_SEND\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_assemble_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_write.py\u001b[0m in \u001b[0;36m_assemble_writes\u001b[0;34m(writes)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mtuples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTASKS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mChannelWriteTupleEntry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mww\u001b[0m \u001b[0;34m:=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m                 \u001b[0mtuples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mww\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mChannelWriteEntry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/graph/state.py\u001b[0m in \u001b[0;36m_get_updates\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m    988\u001b[0m                     \u001b[0merror_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mErrorCode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINVALID_GRAPH_NODE_RETURN_VALUE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m                 )\n\u001b[0;32m--> 990\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidUpdateError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m         \u001b[0;31m# state updaters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidUpdateError\u001b[0m: Expected dict, got <__main__.GraphState object at 0x7cd245aca5a0>\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_GRAPH_NODE_RETURN_VALUE"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "373c74c1"
      },
      "source": [
        "**Reasoning**:\n",
        "The error message indicates that the node `call_tool` is returning a `GraphState` object, but it should be returning a dictionary representing the state updates. Modify the `call_tool` function to return a dictionary.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "OkP-j9Xp7zJy",
        "outputId": "0afdb02e-02be-40b7-926b-8be9b7ec2de4"
      },
      "source": [
        "# Define a node to handle tool calls\n",
        "def call_tool(state: GraphState):\n",
        "    # In a real scenario, this node would execute the tool calls\n",
        "    # and update the state with the results.\n",
        "    print(\"Calling tool...\")\n",
        "    # Return a dictionary with the updates to the state\n",
        "    return {\"messages\": state.messages + [\"Tool called\"], \"next\": \"end\"}\n",
        "\n",
        "# Re-compile the graph\n",
        "app = graph.compile()\n",
        "\n",
        "# Define the initial state\n",
        "initial_state = GraphState()\n",
        "\n",
        "# Invoke the compiled graph\n",
        "result = app.invoke(initial_state)\n",
        "\n",
        "# Print the output\n",
        "print(\"Final State:\", result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidUpdateError",
          "evalue": "Expected dict, got <__main__.GraphState object at 0x7cd245b96240>\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_GRAPH_NODE_RETURN_VALUE",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidUpdateError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-978477714.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Invoke the compiled graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Print the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/main.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[0m\n\u001b[1;32m   3092\u001b[0m         \u001b[0minterrupts\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInterrupt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3093\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3094\u001b[0;31m         for chunk in self.stream(\n\u001b[0m\u001b[1;32m   3095\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3096\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/main.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2677\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch_cached_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2678\u001b[0m                         \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2679\u001b[0;31m                     for _ in runner.tick(\n\u001b[0m\u001b[1;32m   2680\u001b[0m                         \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2681\u001b[0m                         \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_runner.py\u001b[0m in \u001b[0;36mtick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m                 run_with_retry(\n\u001b[0m\u001b[1;32m    168\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                     \u001b[0mretry_policy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_retry.py\u001b[0m in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;31m# run the task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mParentCommand\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONF\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONFIG_KEY_CHECKPOINT_NS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/_internal/_runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m                 \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRunnable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_write.py\u001b[0m in \u001b[0;36m_write\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mwrite\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         ]\n\u001b[0;32m---> 84\u001b[0;31m         self.do_write(\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mwrites\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_write.py\u001b[0m in \u001b[0;36mdo_write\u001b[0;34m(config, writes, allow_passthrough)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;31m# can move this to a finally block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mwrite\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTYPE_SEND\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONF\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONFIG_KEY_SEND\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_assemble_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_write.py\u001b[0m in \u001b[0;36m_assemble_writes\u001b[0;34m(writes)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mtuples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTASKS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mChannelWriteTupleEntry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mww\u001b[0m \u001b[0;34m:=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m                 \u001b[0mtuples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mww\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mChannelWriteEntry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/graph/state.py\u001b[0m in \u001b[0;36m_get_updates\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m    988\u001b[0m                     \u001b[0merror_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mErrorCode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINVALID_GRAPH_NODE_RETURN_VALUE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m                 )\n\u001b[0;32m--> 990\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidUpdateError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m         \u001b[0;31m# state updaters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidUpdateError\u001b[0m: Expected dict, got <__main__.GraphState object at 0x7cd245b96240>\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_GRAPH_NODE_RETURN_VALUE"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99f60940"
      },
      "source": [
        "**Reasoning**:\n",
        "The error persists because the graph is still configured to return the entire `GraphState` object when setting the finish point. The finish point should represent the final state or output, which in this case should be a dictionary reflecting the state after the tool call. Modify the graph definition to explicitly indicate that the output is the state dictionary when setting the finish point.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "-mjPa6G871hs",
        "outputId": "64e4b29d-38eb-49ea-b84b-c466fb7e846d"
      },
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "# Define the state - already defined in a previous cell\n",
        "# class GraphState:\n",
        "#     def __init__(self):\n",
        "#         self.messages = []\n",
        "#         self.tool_calls = None\n",
        "#         self.next = None\n",
        "\n",
        "# Create an instance of StateGraph - already created\n",
        "# graph = StateGraph(GraphState)\n",
        "\n",
        "# Define a node to handle tool calls - already defined and modified\n",
        "def call_tool(state: GraphState):\n",
        "    print(\"Calling tool...\")\n",
        "    return {\"messages\": state.messages + [\"Tool called\"], \"next\": \"end\"}\n",
        "\n",
        "# Add the defined nodes to the StateGraph instance - already done\n",
        "# graph.add_node(\"tool_caller\", call_tool)\n",
        "\n",
        "# Set the entry point of the graph - already done\n",
        "# graph.set_entry_point(\"tool_caller\")\n",
        "\n",
        "# Set the finish point of the graph to return the state dictionary\n",
        "graph.set_finish_point(\"tool_caller\", mapper=lambda x: x.__dict__)\n",
        "\n",
        "# Compile the graph\n",
        "app = graph.compile()\n",
        "\n",
        "# Define the initial state - already done\n",
        "initial_state = GraphState()\n",
        "\n",
        "# Invoke the compiled graph\n",
        "result = app.invoke(initial_state)\n",
        "\n",
        "# Print the output\n",
        "print(\"Final State:\", result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "StateGraph.set_finish_point() got an unexpected keyword argument 'mapper'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4151190994.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Set the finish point of the graph to return the state dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_finish_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tool_caller\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Compile the graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: StateGraph.set_finish_point() got an unexpected keyword argument 'mapper'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f9b591f"
      },
      "source": [
        "**Reasoning**:\n",
        "The error indicates that `set_finish_point` does not accept a `mapper` argument. Looking at the LangGraph documentation, the finish point simply indicates the node where the graph execution stops. The final output is typically the state of the graph at that point. The previous error was likely due to the node itself returning an object instead of a dictionary. Since the `call_tool` node is now returning a dictionary, setting the finish point to this node should work correctly.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "-V4Rydm0733-",
        "outputId": "e8f1ee57-7bdf-4610-e8bd-2995b970e65a"
      },
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "# Define the state - already defined in a previous cell\n",
        "# class GraphState:\n",
        "#     def __init__(self):\n",
        "#         self.messages = []\n",
        "#         self.tool_calls = None\n",
        "#         self.next = None\n",
        "\n",
        "# Create an instance of StateGraph - already created\n",
        "# graph = StateGraph(GraphState)\n",
        "\n",
        "# Define a node to handle tool calls - already defined and modified\n",
        "def call_tool(state: GraphState):\n",
        "    print(\"Calling tool...\")\n",
        "    return {\"messages\": state.messages + [\"Tool called\"], \"next\": \"end\"}\n",
        "\n",
        "# Add the defined nodes to the StateGraph instance - already done\n",
        "# graph.add_node(\"tool_caller\", call_tool)\n",
        "\n",
        "# Set the entry point of the graph - already done\n",
        "# graph.set_entry_point(\"tool_caller\")\n",
        "\n",
        "# Set the finish point of the graph to the tool_caller node\n",
        "graph.set_finish_point(\"tool_caller\")\n",
        "\n",
        "# Compile the graph\n",
        "app = graph.compile()\n",
        "\n",
        "# Define the initial state - already done\n",
        "initial_state = GraphState()\n",
        "\n",
        "# Invoke the compiled graph\n",
        "result = app.invoke(initial_state)\n",
        "\n",
        "# Print the output\n",
        "print(\"Final State:\", result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langgraph.graph.state:Adding an edge to a graph that has already been compiled. This will not be reflected in the compiled graph.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidUpdateError",
          "evalue": "Expected dict, got <__main__.GraphState object at 0x7cd2458ce1e0>\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_GRAPH_NODE_RETURN_VALUE",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidUpdateError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2173031445.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# Invoke the compiled graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# Print the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/main.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[0m\n\u001b[1;32m   3092\u001b[0m         \u001b[0minterrupts\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInterrupt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3093\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3094\u001b[0;31m         for chunk in self.stream(\n\u001b[0m\u001b[1;32m   3095\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3096\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/main.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2677\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch_cached_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2678\u001b[0m                         \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2679\u001b[0;31m                     for _ in runner.tick(\n\u001b[0m\u001b[1;32m   2680\u001b[0m                         \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2681\u001b[0m                         \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_runner.py\u001b[0m in \u001b[0;36mtick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m                 run_with_retry(\n\u001b[0m\u001b[1;32m    168\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                     \u001b[0mretry_policy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_retry.py\u001b[0m in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;31m# run the task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mParentCommand\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONF\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONFIG_KEY_CHECKPOINT_NS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/_internal/_runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m                 \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRunnable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_write.py\u001b[0m in \u001b[0;36m_write\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mwrite\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         ]\n\u001b[0;32m---> 84\u001b[0;31m         self.do_write(\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mwrites\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_write.py\u001b[0m in \u001b[0;36mdo_write\u001b[0;34m(config, writes, allow_passthrough)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;31m# can move this to a finally block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mwrite\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTYPE_SEND\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONF\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONFIG_KEY_SEND\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_assemble_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_write.py\u001b[0m in \u001b[0;36m_assemble_writes\u001b[0;34m(writes)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mtuples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTASKS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mChannelWriteTupleEntry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mww\u001b[0m \u001b[0;34m:=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m                 \u001b[0mtuples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mww\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mChannelWriteEntry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/graph/state.py\u001b[0m in \u001b[0;36m_get_updates\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m    988\u001b[0m                     \u001b[0merror_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mErrorCode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINVALID_GRAPH_NODE_RETURN_VALUE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m                 )\n\u001b[0;32m--> 990\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidUpdateError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m         \u001b[0;31m# state updaters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidUpdateError\u001b[0m: Expected dict, got <__main__.GraphState object at 0x7cd2458ce1e0>\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_GRAPH_NODE_RETURN_VALUE"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99400c4a"
      },
      "source": [
        "**Reasoning**:\n",
        "The error message `InvalidUpdateError: Expected dict, got <__main__.GraphState object at 0x...>` indicates that the state update is not in the expected dictionary format. Although the `call_tool` function is returning a dictionary, the state being passed around in the graph is still the `GraphState` object. The `GraphState` class needs to inherit from `TypedDict` to work correctly with LangGraph's state management. Also, ensure that the state class is defined correctly and used consistently. Let's redefine the `GraphState` class inheriting from `TypedDict` and then re-run the graph.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtRzkRzH76iv",
        "outputId": "0741d4ea-1227-4e19-b066-7aca53f21612"
      },
      "source": [
        "from typing import TypedDict, List, Annotated\n",
        "from langgraph.graph import StateGraph, END\n",
        "import operator\n",
        "\n",
        "# Define the state as a TypedDict\n",
        "class GraphState(TypedDict):\n",
        "    \"\"\"Represents the state of our graph.\"\"\"\n",
        "    messages: Annotated[List[str], operator.add]\n",
        "    tool_calls: Annotated[List[dict] | None, operator.add]\n",
        "    next: str | None\n",
        "\n",
        "# Create an instance of StateGraph\n",
        "graph = StateGraph(GraphState)\n",
        "\n",
        "# Define a node to handle tool calls\n",
        "def call_tool(state: GraphState):\n",
        "    print(\"Calling tool...\")\n",
        "    # The node should return updates to the state as a dictionary\n",
        "    return {\"messages\": [\"Tool called\"], \"next\": \"end\"}\n",
        "\n",
        "# Add the defined nodes to the StateGraph instance\n",
        "graph.add_node(\"tool_caller\", call_tool)\n",
        "\n",
        "# Set the entry point of the graph\n",
        "graph.set_entry_point(\"tool_caller\")\n",
        "\n",
        "# Set the finish point of the graph\n",
        "graph.set_finish_point(\"tool_caller\")\n",
        "\n",
        "# Compile the graph\n",
        "app = graph.compile()\n",
        "\n",
        "# Define the initial state\n",
        "# The initial state should also be a dictionary conforming to GraphState\n",
        "initial_state: GraphState = {\"messages\": [], \"tool_calls\": None, \"next\": None}\n",
        "\n",
        "# Invoke the compiled graph\n",
        "result = app.invoke(initial_state)\n",
        "\n",
        "# Print the output\n",
        "print(\"Final State:\", result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calling tool...\n",
            "Final State: {'messages': ['Tool called'], 'tool_calls': None, 'next': 'end'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4932a19e"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The `langgraph` library was successfully installed, including its dependencies.\n",
        "*   Two dummy tools, `tool1` and `tool2`, were defined using the `@tool` decorator from `langchain_core.tools`.\n",
        "*   A basic LangGraph `StateGraph` was defined with a single node named `tool_caller` which simulates calling a tool.\n",
        "*   Initially, running the graph failed with an `InvalidUpdateError` because the `GraphState` was defined as a custom class instead of inheriting from `typing.TypedDict`, which is required for LangGraph's state management.\n",
        "*   After redefining `GraphState` as a `TypedDict` and providing the initial state as a dictionary, the graph executed successfully.\n",
        "*   The final state of the graph shows the message \"Tool called\" was added, indicating the `call_tool` node was executed.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   Future development should focus on integrating actual tool execution within the `call_tool` node and handling the results to update the graph state appropriately.\n",
        "*   The graph can be expanded to include more complex logic, such as conditional routing based on tool outputs or the addition of an agent node to decide which tool to call.\n"
      ]
    }
  ]
}